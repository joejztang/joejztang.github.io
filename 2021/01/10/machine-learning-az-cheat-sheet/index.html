<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="migrating to raspberry server"><title>machine learning a-z cheat sheet | 130L blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-118615339-1','auto');ga('send','pageview');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">machine learning a-z cheat sheet</h1><a id="logo" href="/.">130L blog</a><p class="description">Cogito, ergo sum</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">machine learning a-z cheat sheet</h1><div class="post-meta">2021-01-10<span> | </span><span class="category"><a href="/categories/machine-learning/">machine learning</a></span></div><a class="disqus-comment-count" data-disqus-identifier="2021/01/10/machine-learning-az-cheat-sheet/" href="/2021/01/10/machine-learning-az-cheat-sheet/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#null-hypothesis-and-p-value"><span class="toc-number">1.</span> <span class="toc-text">null hypothesis and p-value</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Regression-Supervised-Learninng"><span class="toc-number"></span> <span class="toc-text">Regression - [Supervised Learninng]</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression"><span class="toc-number">1.</span> <span class="toc-text">Linear Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dummy-variable-trap-for-linear-regression"><span class="toc-number">1.1.</span> <span class="toc-text">Dummy variable trap for linear regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear-Regression-template"><span class="toc-number">1.2.</span> <span class="toc-text">Linear Regression template</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backward-Elimination"><span class="toc-number">1.3.</span> <span class="toc-text">Backward Elimination</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVR"><span class="toc-number">2.</span> <span class="toc-text">SVR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decision-Tree-Regression"><span class="toc-number">3.</span> <span class="toc-text">Decision Tree Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Random-Forest-and-Ensemble-Learning"><span class="toc-number">4.</span> <span class="toc-text">Random Forest and Ensemble Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#R-Squared-R-2-performation-of-the-regression-model"><span class="toc-number">5.</span> <span class="toc-text">R-Squared (R^2) - performation of the regression model</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Classification-Unsupervised-Learning"><span class="toc-number"></span> <span class="toc-text">Classification - [Unsupervised Learning]</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression"><span class="toc-number">1.</span> <span class="toc-text">Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Confusion-matrix-and-accuracy-score"><span class="toc-number">1.1.</span> <span class="toc-text">Confusion matrix and accuracy score</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-Nearest-Neighbor"><span class="toc-number">2.</span> <span class="toc-text">K Nearest Neighbor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM"><span class="toc-number">3.</span> <span class="toc-text">SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernel-SVM"><span class="toc-number">3.1.</span> <span class="toc-text">Kernel SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kernel-Tricks"><span class="toc-number">3.2.</span> <span class="toc-text">Kernel Tricks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Non-linear-SVR-not-SVM"><span class="toc-number">3.3.</span> <span class="toc-text">Non linear SVR (not SVM)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Naive-Bayes"><span class="toc-number">4.</span> <span class="toc-text">Naive Bayes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Naive-Bayes-Classifier-intuition"><span class="toc-number">4.1.</span> <span class="toc-text">Naive Bayes Classifier intuition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decision-Tree-Classification"><span class="toc-number">5.</span> <span class="toc-text">Decision Tree Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Random-Forest-Classification"><span class="toc-number">6.</span> <span class="toc-text">Random Forest Classification</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Evaluation-of-classification"><span class="toc-number"></span> <span class="toc-text">Evaluation of classification</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Clustering"><span class="toc-number"></span> <span class="toc-text">Clustering</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#K-Means"><span class="toc-number">1.</span> <span class="toc-text">K Means</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means-init-trap"><span class="toc-number">1.1.</span> <span class="toc-text">K means init trap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Means-choosing-the-number-number-of-clusters"><span class="toc-number">1.2.</span> <span class="toc-text">K Means: choosing the number number of clusters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-Mean-Procedures"><span class="toc-number">1.3.</span> <span class="toc-text">K Mean Procedures</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hierarchical-Clustering"><span class="toc-number">2.</span> <span class="toc-text">Hierarchical Clustering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dendrograms"><span class="toc-number">2.1.</span> <span class="toc-text">Dendrograms</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Association-Rule-Learning"><span class="toc-number"></span> <span class="toc-text">Association Rule Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Apriori"><span class="toc-number">1.</span> <span class="toc-text">Apriori</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Eclat"><span class="toc-number">2.</span> <span class="toc-text">Eclat</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reinforcement-Learning"><span class="toc-number"></span> <span class="toc-text">Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary-of-UCB-and-Thompson-Sampling"><span class="toc-number">1.</span> <span class="toc-text">Summary of UCB and Thompson Sampling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Upper-Confidence-Bound"><span class="toc-number">2.</span> <span class="toc-text">Upper Confidence Bound</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Thompson-Sampling"><span class="toc-number">3.</span> <span class="toc-text">Thompson Sampling</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NLP"><span class="toc-number"></span> <span class="toc-text">NLP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Types-of-Natural-Language-Processing"><span class="toc-number">1.</span> <span class="toc-text">Types of Natural Language Processing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Classical-v-s-deep-learning-models"><span class="toc-number">2.</span> <span class="toc-text">Classical v.s. deep learning models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bag-of-Words"><span class="toc-number">3.</span> <span class="toc-text">Bag of Words</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Intuition"><span class="toc-number">3.1.</span> <span class="toc-text">Intuition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Assumptions"><span class="toc-number">3.2.</span> <span class="toc-text">Assumptions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limitations"><span class="toc-number">3.3.</span> <span class="toc-text">Limitations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#choice-of-classifier-using-bag-of-words"><span class="toc-number">3.4.</span> <span class="toc-text">choice of classifier using bag of words</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Neural-Network"><span class="toc-number"></span> <span class="toc-text">Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#simplified-training-process"><span class="toc-number">1.</span> <span class="toc-text">simplified training process</span></a></li></ol></div></div><div class="post-content"><h2 id="null-hypothesis-and-p-value"><a href="#null-hypothesis-and-p-value" class="headerlink" title="null hypothesis and p-value"></a>null hypothesis and p-value</h2><p><a href="200~https://towardsdatascience.com/null-hypothesis-and-the-p-value-fdc129db6502">very useful post</a></p>
<p>so to summarize somewhat into my own words:</p>
<ol>
<li><p>null hypothesis is</p>
<blockquote>
<p>The null hypothesis, H0 is the commonly accepted fact; it is the opposite of the alternate hypothesis. Researchers work to reject, nullify or disprove the null hypothesis.</p>
</blockquote>
<p>this is coming from <a target="_blank" rel="noopener" href="https://www.statisticshowto.com/probability-and-statistics/null-hypothesis/">here</a></p>
</li>
</ol>
<p>so how do we “nullify” the hypothesis? we are using p-value</p>
<ol start="2">
<li><p>we are using p-value to nullify the hypothesis</p>
<p>the common chose p-value is 0.05, if the calculated p-value is less than 0.05, then it’s nullified; otherwise, the hypothesis stays.</p>
</li>
</ol>
<h1 id="Regression-Supervised-Learninng"><a href="#Regression-Supervised-Learninng" class="headerlink" title="Regression - [Supervised Learninng]"></a>Regression - [Supervised Learninng]</h1><p>Regression is a useful tool to predict a continuous number.</p>
<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>Before building a linear regression model, there is a caveat</p>
<blockquote>
<p>Assumptions of Linear Regression:</p>
<ol>
<li>linearity</li>
<li><a target="_blank" rel="noopener" href="https://www.google.com/search?q=homoscedasticity&rlz=1C5CHFA_enUS651US651&oq=homosec&aqs=chrome.2.69i57j0i457j0i10i433j0j0i10l4.5514j0j9&sourceid=chrome&ie=UTF-8">homoscedasticity</a></li>
<li>multivariate nomarlity</li>
<li>independence of errors</li>
<li>lack of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a></li>
</ol>
</blockquote>
<h3 id="Dummy-variable-trap-for-linear-regression"><a href="#Dummy-variable-trap-for-linear-regression" class="headerlink" title="Dummy variable trap for linear regression"></a>Dummy variable trap for linear regression</h3><p>take an example here, say we have a model like this <code>y = b0 + b1*x1 + b2*x2 + b3*x3</code>, and we have one more column we want to take into consideration, say it’s state.</p>
<p>we have two distinct states inside the column, they are “New York” and “California”. by encoding, we can say it it’s NY, we will have a vector like <code>[1, 0]</code>, so for CA, we will have <code>[0, 1]</code>. just a representation.</p>
<p>so, it comes to an question that how should we include them, is it <code>y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1</code> or is it <code>y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1 + b5*D2</code>?</p>
<p>the answer is the first one <code>y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1</code>.</p>
<p>the reason is that, we know state is mutual exclusive, meaning it couldn’t be both NY and CA, so naturally <code>D1+D2 = 1</code>. so if we introduce both D1 and D2 in the function we will violate “lack of multicollinearity” assumption.</p>
<h3 id="Linear-Regression-template"><a href="#Linear-Regression-template" class="headerlink" title="Linear Regression template"></a>Linear Regression template</h3><ol>
<li>import the libraries</li>
<li>import the dataset</li>
<li>encoding categorical data: hot-encoding will always come to the first columns</li>
<li>splitting the dataset into training set and test set</li>
<li>training the linear regression model on training set</li>
<li>predict the test dataset result</li>
</ol>
<h3 id="Backward-Elimination"><a href="#Backward-Elimination" class="headerlink" title="Backward Elimination"></a>Backward Elimination</h3><p>the following code snippet copied from <a target="_blank" rel="noopener" href="https://www.udemy.com/course/machinelearning/">machine learnin a-z</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backwardElimination</span>(<span class="params">x, sl</span>):</span></span><br><span class="line">    numVars = <span class="built_in">len</span>(x[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, numVars):</span><br><span class="line">        regressor_OLS = sm.OLS(y, x).fit()</span><br><span class="line">        maxVar = <span class="built_in">max</span>(regressor_OLS.pvalues).astype(<span class="built_in">float</span>)</span><br><span class="line">        <span class="keyword">if</span> maxVar &gt; sl:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, numVars - i):</span><br><span class="line">                <span class="keyword">if</span> (regressor_OLS.pvalues[j].astype(<span class="built_in">float</span>) == maxVar):</span><br><span class="line">                    x = np.delete(x, j, <span class="number">1</span>)</span><br><span class="line">    regressor_OLS.summary()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a>SVR</h2><p>SVR vs SVM, both have support vector in the names, but they are for different purposes.</p>
<p>SVR stands for support vector regression, in essence it’s a regression algorithm, SVR will works forr continuous values</p>
<p>whereas SVM is a classification algorithm.</p>
<p>As name indicated, SVR could be used to predict.</p>
<p>template for SVR:</p>
<ol>
<li>import libs</li>
<li>import dataset</li>
<li>feature scaling</li>
<li>train the SVR model on the whole dataset</li>
<li>predict new result</li>
<li>visualize the result</li>
</ol>
<p>Noted number 3. why do a feature scaling? Because in machine learning algorithms, it’s essential to calculate <em>distance between data</em>.<br>If not scale, the feature with higher value range starts dominating.<br>check details <a target="_blank" rel="noopener" href="https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35">here</a></p>
<p>And remember to call <code>inverse_transfrom</code> method after you got the predictions. Make sure to use the right scaler.</p>
<h2 id="Decision-Tree-Regression"><a href="#Decision-Tree-Regression" class="headerlink" title="Decision Tree Regression"></a>Decision Tree Regression</h2><p>Decision tree can be used to both regression and classification.</p>
<p>The essence of decision tree regression is to split the tree, calculate the entropy for each leaf node in order to find the most important attribute.</p>
<p>So, that said, we don’t need to do feature scaling on decision tree model.</p>
<p>Noted that decision tree regression is not suitable for single feature model(?), it will perform better for multi feature model.</p>
<p>The same for random forest regression, no feature scaling</p>
<h2 id="Random-Forest-and-Ensemble-Learning"><a href="#Random-Forest-and-Ensemble-Learning" class="headerlink" title="Random Forest and Ensemble Learning"></a>Random Forest and Ensemble Learning</h2><p>Random forest is one kind of ensemble learning. The essence of ensemble learning, random forest specifically in our case, is to enlarge the hypothesis, and combine hypotheses later on.</p>
<p>Random forest seems more adaptable on classification because by utilizing the essence of ensemble learning, we hope it will decrease the classification: it will be unlikely for all 5 hypotheses to misclassify than one; in other word, we are more safe to trust the all 5 “misclassifies”</p>
<h2 id="R-Squared-R-2-performation-of-the-regression-model"><a href="#R-Squared-R-2-performation-of-the-regression-model" class="headerlink" title="R-Squared (R^2) - performation of the regression model"></a>R-Squared (R^2) - performation of the regression model</h2><p>Simple definition of R squared.</p>
<p><code>SS res = SUM(yi - y^i)^2</code><br><code>SS tot = SUM(yi - yavg)^2</code></p>
<p><code>R^2 = 1-SS res/SS tot</code></p>
<p>R^2 ideally would be from 0 to 1, the more to 1 the better, the more to 1 the more fit to the data.</p>
<p><em>Adjusted R Squared</em></p>
<p>The intuition that we have an adjusted R squared is that because it (original R squared) will never decrease if add more feature into the hypothesis</p>
<p><code>Adj R^2 = 1-(1-R^2)(n-1)/(n-p-1_)</code></p>
<p>where <code>p - number of regressors</code> and <code>n - sample size</code></p>
<p>R squared is also a criteria to choose which model to use.</p>
<h1 id="Classification-Unsupervised-Learning"><a href="#Classification-Unsupervised-Learning" class="headerlink" title="Classification - [Unsupervised Learning]"></a>Classification - [Unsupervised Learning]</h1><p>Classification is a powerful tool to predict a category.</p>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>Logistic regression template</p>
<ol>
<li>import libs</li>
<li>import dataset</li>
<li>splitting the dataset into training and test set</li>
<li>feature scaling</li>
<li>training (fit) the logistic regression model on training dataset</li>
<li>predict a new result</li>
<li>predicting the test set result</li>
<li>making the confusion matrix</li>
<li>visualize both train and test set results</li>
</ol>
<p>1-4 are data preprocessing, 5 is fit step, 6,7 are prediciton and rest are visualization.</p>
<h3 id="Confusion-matrix-and-accuracy-score"><a href="#Confusion-matrix-and-accuracy-score" class="headerlink" title="Confusion matrix and accuracy score"></a>Confusion matrix and accuracy score</h3><p>In <code>sklearn.metrics</code>, but what is underneath?</p>
<blockquote>
<p>A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known.</p>
</blockquote>
<p>please go to <a target="_blank" rel="noopener" href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology">dataschool</a></p>
<p><img src="https://www.dataschool.io/content/images/2015/01/confusion_matrix_simple2.png" alt="confustion_matrix"></p>
<h2 id="K-Nearest-Neighbor"><a href="#K-Nearest-Neighbor" class="headerlink" title="K Nearest Neighbor"></a>K Nearest Neighbor</h2><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>back to SVR, in stead of using it to predict continous numbers, we use it to classify.</p>
<p>Some terms:</p>
<ul>
<li>hyperplane</li>
<li>support vectors</li>
<li>maximum margin</li>
</ul>
<p>SVM is somewhat special because the support vectors it uses to construct margin are the “edge cases”. Poeple find it predicts better than non SVM algo often because it uses the edge cases.</p>
<h3 id="Kernel-SVM"><a href="#Kernel-SVM" class="headerlink" title="Kernel SVM"></a>Kernel SVM</h3><p>If the data is linearly separable, then use linear SVM; otherwise, we cannot use linear SVM.</p>
<p>Then the question becomes what to use?</p>
<p>Some intuition that classify some non linear dataset using SVM is to make dataset to a higher dimension.</p>
<p>The pros of doing that is it sometimes is easy to find the hyperplane when in a higher dimension; the cons is computing intensive.</p>
<p>After using SVM solving it in a higher dimension, then map it back.</p>
<p>Now, after the intuition, how do we actually do it?</p>
<p>Since we are under Kernel Function, yes, by applying kernel functions, basically we are doing those.</p>
<p>An <a target="_blank" rel="noopener" href="https://medium.com/@suvigya2001/the-gaussian-rbf-kernel-in-non-linear-svm-2fb1c822aae0">example</a> here. In the middle of the post, it generates a nice graph. First using kernel function in this case RBF pull data from 2d to 3d, classify them, and then push back data from 3d to 2d. Contour will be left on 2d plate. (kernel trick???)</p>
<p>Some common kernel functions that we might use including:</p>
<ul>
<li>Gaussian RBF</li>
<li>Polynomial kernel function</li>
<li>Sigmoid kernel function</li>
</ul>
<h3 id="Kernel-Tricks"><a href="#Kernel-Tricks" class="headerlink" title="Kernel Tricks"></a>Kernel Tricks</h3><p>According to this <a target="_blank" rel="noopener" href="https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d">post</a>, keneral trick is so important that it bridges linearity to non-linearity.</p>
<h3 id="Non-linear-SVR-not-SVM"><a href="#Non-linear-SVR-not-SVM" class="headerlink" title="Non linear SVR (not SVM)"></a>Non linear SVR (not SVM)</h3><p>By using kernel trick, or pull data into higher demensions-find max and min hyperplane-push higher demension back to original, we could find a tube (in this time it’s a curved cube instead of two lines) </p>
<p>How do we do that? Kernel tricks. So kernel tricks is very important.</p>
<h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><p>The following is the Bayes Thereom</p>
<p>\( {P(A|B)}{P(B)} = {P(B|A)}{P(A)}\)</p>
<h3 id="Naive-Bayes-Classifier-intuition"><a href="#Naive-Bayes-Classifier-intuition" class="headerlink" title="Naive Bayes Classifier intuition"></a>Naive Bayes Classifier intuition</h3><p>Plan to attack a problem, have to borrow Bayes Thereom again</p>
<p>\( {P(A|B)} = \frac{P(B|A)*P(A)}{P(B)} \)</p>
<p>We are now giving each element a new name:</p>
<ol>
<li>\( {P(A)} \), the Prior Probablility</li>
<li>\( {P(B)} \), the Marginal Likelihood</li>
<li>\( {P(B|A)} \), the Likelihood</li>
<li>\( {P(A|B)} \), the Posterior Probablility</li>
</ol>
<p>When attack a problem, we want to calculate elements <em>in order</em>.</p>
<p>A concrete example, we assign <code>B</code> as the features dataset <code>X</code>, and given <code>X</code>, we have multiple categories that can classify to; in other words, <code>A</code> can assign to <code>cat1</code>, <code>cat2</code>, etc.</p>
<p>So the function now becomes:</p>
<p>\( {P(cat1|X)} = \frac{P(X|cat1)*P(cat1)}{P(X)} \)</p>
<p>If we want to predict a data point, <code>x</code>, we need to calculate all \( {P(cat1|X)} \) and \( {P(cat2|X)} \), at last compare them and assign to the correct cat</p>
<p>Noted for number2, the marginal likelihood. Calculating that likelihood will introduce a “margin”, which will be having the similar features of the “newly added point/the point to be predicted”. So feels like choosing that “margin” is an critical work during the calulation.</p>
<h2 id="Decision-Tree-Classification"><a href="#Decision-Tree-Classification" class="headerlink" title="Decision Tree Classification"></a>Decision Tree Classification</h2><p>Classification and Regression Trees (CART)</p>
<p>Decision Tree recently was dying, but some other algorithm utilize the simplicity of the decision trees to make it reborn.</p>
<p>They are Random Forest, Gradient Boosting, etc.</p>
<h2 id="Random-Forest-Classification"><a href="#Random-Forest-Classification" class="headerlink" title="Random Forest Classification"></a>Random Forest Classification</h2><p>Random Forest is some kind of ensemble learning.</p>
<h1 id="Evaluation-of-classification"><a href="#Evaluation-of-classification" class="headerlink" title="Evaluation of classification"></a>Evaluation of classification</h1><ul>
<li>confusion matrix: In general, false positive is more like a warning, whereas false negative is like the real “error”.<pre><code>              In diagnal, they are the right prediction.</code></pre>
</li>
<li>Cumulative Accuracy Profile (CAP) vs. Receiver Operating Characteristic (ROC)<ul>
<li><a target="_blank" rel="noopener" href="https://community.tibco.com/wiki/gains-vs-roc-curves-do-you-understand-difference?exp=cloud&pref=off">some explains</a></li>
<li>when doing cap analysis, this guy introduce AR <img src="/images/ar.png" alt="ar"> and some criteria <img src="/images/cap_analysis.png" alt="cri"></li>
</ul>
</li>
</ul>
<h1 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h1><h2 id="K-Means"><a href="#K-Means" class="headerlink" title="K Means"></a>K Means</h2><ol>
<li>choose the number K of clusters</li>
<li>select at random K points. the centroids (not necessarily from your dataset)</li>
<li>assign each data point to the closest centroid, that forms k clusters (calculate distance to the centroid)</li>
<li>compute and place the new centroid of each cluster. (average of data within the cluster)</li>
<li>reassign each data point to the new closest centroid.</li>
</ol>
<p>if any <strong>reassignment</strong> took place, go to 4 again.</p>
<p>otherwise, the alg is finished.</p>
<h3 id="K-means-init-trap"><a href="#K-means-init-trap" class="headerlink" title="K means init trap"></a>K means init trap</h3><p>different select of centroids will result in false consequence.</p>
<p>use <strong>Kmean++</strong> algorithm.</p>
<h3 id="K-Means-choosing-the-number-number-of-clusters"><a href="#K-Means-choosing-the-number-number-of-clusters" class="headerlink" title="K Means: choosing the number number of clusters"></a>K Means: choosing the number number of clusters</h3><p>\( WCSS = \sum_{Pi in Cluster j}^{total i} \sum_{j}^{total j} distance(Pi, Cj)^2 \) </p>
<p>WCSS will be decreasing continously and to 0 if every point has a centroid of its own.</p>
<p>What should we do to optimize it? Draw WCSS vs Number of Clusters and using elbow method to decide the number of clusters.</p>
<h3 id="K-Mean-Procedures"><a href="#K-Mean-Procedures" class="headerlink" title="K Mean Procedures"></a>K Mean Procedures</h3><ol>
<li>import libs</li>
<li>import datasets</li>
<li>elbow method to find number of clusters</li>
<li>train the K Means model on the dataset</li>
<li>visualization</li>
</ol>
<h2 id="Hierarchical-Clustering"><a href="#Hierarchical-Clustering" class="headerlink" title="Hierarchical Clustering"></a>Hierarchical Clustering</h2><p>We majorly focus on the agglomerative clustering rather than divisive here</p>
<ol>
<li>Make each data point a single-point cluster -&gt; forms N clusters</li>
<li>take the two closest clusters and make them one cluster</li>
<li>repeat until there is only one cluster</li>
</ol>
<p>How to define the distance between the two clusters?</p>
<p>There are several options: 1. closest points, 2. furthest points, 3. average distance, 4. distance between centoids.<br>As long as it is consistent, then it’s fine</p>
<p>How to decide the best cluster numbers?</p>
<h3 id="Dendrograms"><a href="#Dendrograms" class="headerlink" title="Dendrograms"></a>Dendrograms</h3><p><a target="_blank" rel="noopener" href="https://medium.com/@displayr/what-is-a-dendrogram-9de4fc1f131e">what is a dendrogram</a></p>
<p>And the quickest way making a decision is to see which vertical line between the two horizontal line is the longest.</p>
<p>like this example below</p>
<p><img src="/images/dendrogram.png" alt="dendrogram"></p>
<p>we see the correct answer for this is 3 clusters, not 2 clusters because the red line has the largest distance.</p>
<h1 id="Association-Rule-Learning"><a href="#Association-Rule-Learning" class="headerlink" title="Association Rule Learning"></a>Association Rule Learning</h1><h2 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h2><p>\( {lift(M_1 -&gt; M_2)} = \frac{confidence(M_1 -&gt; M_2)}{support(M_2)} \)</p>
<p>A practical usage of this alg would be frequent purchased together. (I am guessings)</p>
<h2 id="Eclat"><a href="#Eclat" class="headerlink" title="Eclat"></a>Eclat</h2><p>Only <code>support</code> matters. Still use <code>apriori</code> model to give rules, but using eclat method to analyze.</p>
<h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><p>What is reinforcement learning?</p>
<blockquote>
<p>Reinforcement Learning is a subset of machine learning. It enables an agent to learn through the consequences of actions in a specific environment. It can be used to teach a robot new tricks, for example.<br>– towardsdatascience.com</p>
</blockquote>
<h2 id="Summary-of-UCB-and-Thompson-Sampling"><a href="#Summary-of-UCB-and-Thompson-Sampling" class="headerlink" title="Summary of UCB and Thompson Sampling"></a>Summary of UCB and Thompson Sampling</h2><ul>
<li>UCB is a deterministic algorithm whereas Thompson Sampling is a probalistic algorithm.</li>
</ul>
<h2 id="Upper-Confidence-Bound"><a href="#Upper-Confidence-Bound" class="headerlink" title="Upper Confidence Bound"></a>Upper Confidence Bound</h2><p>Coming from <a target="_blank" rel="noopener" href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html">multi-armed bandit</a> problem.</p>
<p>Some simplified steps for UCB alg:</p>
<ol>
<li>At each round n, we consider two numbers for each ad i:<ul>
<li>\( N_i(n) \) - the number of times the ad i was selected up to round n</li>
<li>\( R_i(n) \) - the sum of rewards of the ad i up to round n</li>
</ul>
</li>
<li>From these two numbers we compute:<ul>
<li>the average reward of ad i up to round n \( \bar{r}_i(n) = \frac{R_i(n)}{N_i(n)} \)</li>
<li>the confidence interval \( [\bar{r}_i(n) - \Delta_i(n), \bar{r}_i(n) + \Delta_i(n)] \) at round n with \( \Delta_i(n) = \sqrt{\frac{3log(n)}{2N_i(n)}} \)</li>
</ul>
</li>
<li>We select the ad i that has the maximum UCB \( \bar{r}_i(n) + \Delta_i(n) \)</li>
</ol>
<h2 id="Thompson-Sampling"><a href="#Thompson-Sampling" class="headerlink" title="Thompson Sampling"></a>Thompson Sampling</h2><p>some simplified steps for Thompson Sampling</p>
<ol>
<li>At each round n, we consider two numbers for each ad i:<ul>
<li>\( N_i^1(n) \) - the number of times the ad i got reward 1 up to round n.</li>
<li>\( N_i^0(n) \) - the number of times the ad i got reward 0 up to round n.</li>
</ul>
</li>
<li>For each ad i, we take a random draw from the distribution \( \theta_i(n) = \beta(N_i^1(n)+1, N_i^0(n)+1) \)</li>
<li>We select the ad that has the highest \( \theta_i(n) \)</li>
</ol>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><h2 id="Types-of-Natural-Language-Processing"><a href="#Types-of-Natural-Language-Processing" class="headerlink" title="Types of Natural Language Processing"></a>Types of Natural Language Processing</h2><p>Once there were classic NLP, and there were deep learning.</p>
<p>Now we have the intersection called DNLP, within that intersection, we have seq2seq</p>
<p>TLDR: classic NLP            DNLP(including seq2seq)                 Deep Learning</p>
<h2 id="Classical-v-s-deep-learning-models"><a href="#Classical-v-s-deep-learning-models" class="headerlink" title="Classical v.s. deep learning models"></a>Classical v.s. deep learning models</h2><ul>
<li>classical examples:<ul>
<li>if/else rules used to be used on chatbot</li>
<li>some audio frequency analysis</li>
<li>bag of words used for classification</li>
</ul>
</li>
<li>not so classical –&gt; deep learning<ul>
<li>CNN for text recognition used for classification</li>
<li>seq2seq can be used for many applications.</li>
</ul>
</li>
</ul>
<h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h2><blockquote>
<p>The bag-of-words model is a simplifying <strong>representation</strong> used in natural language processing and information retrieval (IR).<br>– wikipedia</p>
</blockquote>
<h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>One specific sentence is represented by a array <code>[0, 0, ..., 0]</code>, where <code>array[0]</code> represents the start of the sentence (SOS), <code>array[1]</code> represents the end of the sentence (EOS), <code>array[-1]</code> represents the special words.</p>
<p>Usually array is with length of 20000 elements long.</p>
<p>The number of 20000 is because of</p>
<blockquote>
<p>Most adult native test-takers knows ranging from 20000-35000<br>— The Economist</p>
</blockquote>
<p>By feeding training data, some sentences prepared are used to train the model.</p>
<h3 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h3><ul>
<li>position of the words in the document doesn’t matter.</li>
</ul>
<h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>Limitations on bag of words algorithm is that it can only answer Y/N question.</p>
<h3 id="choice-of-classifier-using-bag-of-words"><a href="#choice-of-classifier-using-bag-of-words" class="headerlink" title="choice of classifier using bag of words"></a>choice of classifier using bag of words</h3><p>see from <a target="_blank" rel="noopener" href="https://theflyingmantis.medium.com/text-classification-in-nlp-naive-bayes-a606bf419f8c">here</a></p>
<ul>
<li>No data –&gt; handwritten rules.</li>
<li>less training data –&gt; naive bayes</li>
<li>reasonable amount –&gt; SVM &amp; Logical regression. Decision trees maybe.</li>
</ul>
<h1 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h1><h2 id="simplified-training-process"><a href="#simplified-training-process" class="headerlink" title="simplified training process"></a>simplified training process</h2><ol>
<li>Randomly initialize the weights to small numbers close to 0 (but not 0)</li>
<li>input the first observation of your dataset in the input layer, each feature in one input node.</li>
<li>forward-propagation, getting result y.</li>
<li>compare the predicted result to the actual result. measure the generated error.</li>
<li>back propagation. <strong>update the weights according to how much they are responsible for the error.</strong> the learning rate decides by how much we update the weights.</li>
<li>repeat 1-5 update the weights after each observation (<strong>Reinforcement Learning</strong>). if update the weigths only after a batch of observations (<strong>Batch Learning</strong>).</li>
<li>When the whole traning set passed thru the ANN, that makes an <strong>epoch</strong>. Redo more epochs.</li>
</ol>
</div><div class="tags"><a href="/tags/cheat-sheet/"><i class="fa fa-tag"></i>cheat sheet</a></div><div class="post-nav"><a class="pre" href="/2021/03/30/python-unittest-mock-and-pytest-monkeypatch/">python unittest mock and pytest monkeypatch</a><a class="next" href="/2020/12/08/code/224-Basic-Calculator/">224. Basic Calculator</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://example.com/2021/01/10/machine-learning-az-cheat-sheet/';
    this.page.identifier = '2021/01/10/machine-learning-az-cheat-sheet/';
    this.page.title = 'machine learning a-z cheat sheet';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//joejztang.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//joejztang.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://joejztang.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/alg-summary/">Alg summary</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/alison-photography/">Alison photography</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/c/">C#</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/dp/">DP</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/django/">Django</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">Hexo</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">Java</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">Math</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/math/linear-algebra/">Linear Algebra</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/r/">R</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/sql-server/">SQL Server</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/scrapy/">Scrapy</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/selfnotes/">SelfNotes</a><span class="category-list-count">30</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/unit-test/">Unit Test</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/apache-beam/">apache beam</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/big-data/">big data</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cheat-sheet/">cheat sheet</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/css/">css</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-structure/">data structure</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/database/">database</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/elasticsearch/">elasticsearch</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/flask/">flask</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/graph/">graph</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/javascript/">javascript</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/jenkins/">jenkins</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/language-common/">language common</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/nodejs/">nodejs</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pandas/">pandas</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/photoshop/">photoshop</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/php/">php</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/reactjs/">reactjs</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/security/">security</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/server/">server</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/recommender-system/" style="font-size: 15px;">recommender system</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/https/" style="font-size: 15px;">https</a> <a href="/tags/cheat-sheet/" style="font-size: 15px;">cheat sheet</a> <a href="/tags/logging/" style="font-size: 15px;">logging</a> <a href="/tags/conda/" style="font-size: 15px;">conda</a> <a href="/tags/venv/" style="font-size: 15px;">venv</a> <a href="/tags/unittest/" style="font-size: 15px;">unittest</a> <a href="/tags/cheating-sheet/" style="font-size: 15px;">cheating sheet</a> <a href="/tags/recursion/" style="font-size: 15px;">recursion</a> <a href="/tags/cte/" style="font-size: 15px;">CTE</a> <a href="/tags/sql-server/" style="font-size: 15px;">SQL Server</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/keras/" style="font-size: 15px;">keras</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/elk/" style="font-size: 15px;">elk</a> <a href="/tags/google-cloud/" style="font-size: 15px;">google cloud</a> <a href="/tags/graph-theory/" style="font-size: 15px;">graph theory</a> <a href="/tags/hexo-404-issue/" style="font-size: 15px;">hexo 404 issue</a> <a href="/tags/seo/" style="font-size: 15px;">seo</a> <a href="/tags/hexo-%E4%BC%98%E5%8C%96/" style="font-size: 15px;">hexo 优化</a> <a href="/tags/kubernetes/" style="font-size: 15px;">kubernetes</a> <a href="/tags/java-io/" style="font-size: 15px;">Java IO</a> <a href="/tags/java-8-streams/" style="font-size: 15px;">java 8 streams</a> <a href="/tags/java-lambda/" style="font-size: 15px;">java lambda</a> <a href="/tags/alg/" style="font-size: 15px;">alg</a> <a href="/tags/binary-search/" style="font-size: 15px;">binary search</a> <a href="/tags/pandas/" style="font-size: 15px;">pandas</a> <a href="/tags/photography/" style="font-size: 15px;">photography</a> <a href="/tags/php-tools/" style="font-size: 15px;">php tools</a> <a href="/tags/debug-trick/" style="font-size: 15px;">debug trick</a> <a href="/tags/heapq/" style="font-size: 15px;">heapq</a> <a href="/tags/circular-imports/" style="font-size: 15px;">circular imports</a> <a href="/tags/mock/" style="font-size: 15px;">mock</a> <a href="/tags/pytest/" style="font-size: 15px;">pytest</a> <a href="/tags/react-redux/" style="font-size: 15px;">react-redux</a> <a href="/tags/javascript/" style="font-size: 15px;">javascript</a> <a href="/tags/react/" style="font-size: 15px;">react</a> <a href="/tags/apache/" style="font-size: 15px;">apache</a> <a href="/tags/redux/" style="font-size: 15px;">redux</a> <a href="/tags/regular-expression/" style="font-size: 15px;">regular expression</a> <a href="/tags/python3/" style="font-size: 15px;">python3</a> <a href="/tags/template/" style="font-size: 15px;">template</a> <a href="/tags/java-8-io/" style="font-size: 15px;">Java 8 IO</a> <a href="/tags/idea/" style="font-size: 15px;">idea</a> <a href="/tags/whim/" style="font-size: 15px;">whim</a> <a href="/tags/hexo-math-equations/" style="font-size: 15px;">hexo math equations</a> <a href="/tags/jenkins/" style="font-size: 15px;">jenkins</a> <a href="/tags/amazon/" style="font-size: 15px;">amazon</a> <a href="/tags/tag/" style="font-size: 15px;">tag</a> <a href="/tags/dp/" style="font-size: 15px;">dp</a> <a href="/tags/ood/" style="font-size: 15px;">ood</a> <a href="/tags/epic/" style="font-size: 15px;">Epic</a> <a href="/tags/sql/" style="font-size: 15px;">sql</a> <a href="/tags/frequent/" style="font-size: 15px;">frequent</a> <a href="/tags/backtracking/" style="font-size: 15px;">backtracking</a> <a href="/tags/inversion-count/" style="font-size: 15px;">inversion count</a> <a href="/tags/code/" style="font-size: 15px;">code</a> <a href="/tags/tbd/" style="font-size: 15px;">tbd</a> <a href="/tags/google/" style="font-size: 15px;">google</a> <a href="/tags/linkedlist/" style="font-size: 15px;">linkedlist</a> <a href="/tags/sliding-window/" style="font-size: 15px;">sliding window</a> <a href="/tags/tree/" style="font-size: 15px;">tree</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%AB%98%E9%A2%91/" style="font-size: 15px;">面试高频</a> <a href="/tags/bfs/" style="font-size: 15px;">BFS</a> <a href="/tags/dfs/" style="font-size: 15px;">DFS</a> <a href="/tags/two-pointer/" style="font-size: 15px;">two pointer</a> <a href="/tags/rabin-karp/" style="font-size: 15px;">rabin-karp</a> <a href="/tags/graph/" style="font-size: 15px;">graph</a> <a href="/tags/stack/" style="font-size: 15px;">stack</a> <a href="/tags/bst/" style="font-size: 15px;">BST</a> <a href="/tags/twitter/" style="font-size: 15px;">twitter</a> <a href="/tags/iterative/" style="font-size: 15px;">iterative</a> <a href="/tags/two-pointers/" style="font-size: 15px;">two pointers</a> <a href="/tags/ood/" style="font-size: 15px;">OOD</a> <a href="/tags/io/" style="font-size: 15px;">io</a> <a href="/tags/object-oriented-design/" style="font-size: 15px;">Object Oriented Design</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/10/28/code/298-Binary-Tree-Longest-Consecutive-Sequence/">298. Binary Tree Longest Consecutive Sequence</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/code/187-Repeated-DNA-Sequences/">187. Repeated DNA Sequences</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/23/code/392-Is-Subsequence/">392. Is Subsequence</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/23/code/301-remove-invalid-parentheses/">301. Remove Invalid Parentheses</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/19/code/267-Palindrome-Permutation-II/">267. Palindrome Permutation II</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/19/code/132-Palindrome-Partitioning-II/">132. Palindrome Partitioning II</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/18/code/131-Palindrome-Partitioning/">131. Palindrome Partitioning</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/14/code/9-Palindrome-Number/">9. Palindrome Number</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/13/code/159-Longest-Substring-with-At-Most-Two-Distinct-Characters/">159. Longest Substring with At Most Two Distinct Characters</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/13/code/395-Longest-Substring-with-At-Least-K-Repeating-Characters/">395. Longest Substring with At Least K Repeating Characters</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//joejztang.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">130L blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=0.0.0"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>