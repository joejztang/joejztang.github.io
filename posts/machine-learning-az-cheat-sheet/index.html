<!doctype html><html lang=en><head><title>machine learning a-z cheat sheet Â· 130l</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="130l"><meta name=description content="
    Table of Contents
    
  
    null hypothesis and p-value
  

  
    Linear Regression
      
        Dummy variable trap for linear regression
        Linear Regression template
        Backward Elimination
      
    
    SVR
    Decision Tree Regression
    Random Forest and Ensemble Learning
    R-Squared (R^2) - performation of the regression model
  

  
    Logistic Regression
      
        Confusion matrix and accuracy score
      
    
    K Nearest Neighbor
    SVM
      
        Kernel SVM
        Kernel Tricks
        Non linear SVR (not SVM)
      
    
    Naive Bayes
      
        Naive Bayes Classifier intuition
      
    
    Decision Tree Classification
    Random Forest Classification
  

  
    K Means
      
        K means init trap
        K Means: choosing the number number of clusters
        K Mean Procedures
      
    
    Hierarchical Clustering
      
        Dendrograms
      
    
  

  
    Apriori
    Eclat
  

  
    Summary of UCB and Thompson Sampling
    Upper Confidence Bound
    Thompson Sampling
  

  
    Types of Natural Language Processing
    Classical v.s. deep learning models
    Bag of Words
      
        Intuition
        Assumptions
        Limitations
        choice of classifier using bag of words
      
    
  

  
    simplified training process
  




  null hypothesis and p-value
  
    
    Link to heading
  

very useful post"><meta name=keywords content="blog,science,technology,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="machine learning a-z cheat sheet"><meta name=twitter:description content="Table of Contents null hypothesis and p-value Linear Regression Dummy variable trap for linear regression Linear Regression template Backward Elimination SVR Decision Tree Regression Random Forest and Ensemble Learning R-Squared (R^2) - performation of the regression model Logistic Regression Confusion matrix and accuracy score K Nearest Neighbor SVM Kernel SVM Kernel Tricks Non linear SVR (not SVM) Naive Bayes Naive Bayes Classifier intuition Decision Tree Classification Random Forest Classification K Means K means init trap K Means: choosing the number number of clusters K Mean Procedures Hierarchical Clustering Dendrograms Apriori Eclat Summary of UCB and Thompson Sampling Upper Confidence Bound Thompson Sampling Types of Natural Language Processing Classical v.s. deep learning models Bag of Words Intuition Assumptions Limitations choice of classifier using bag of words simplified training process null hypothesis and p-value Link to heading very useful post"><meta property="og:url" content="https://joejztang.github.io/posts/machine-learning-az-cheat-sheet/"><meta property="og:site_name" content="130l"><meta property="og:title" content="machine learning a-z cheat sheet"><meta property="og:description" content="Table of Contents null hypothesis and p-value Linear Regression Dummy variable trap for linear regression Linear Regression template Backward Elimination SVR Decision Tree Regression Random Forest and Ensemble Learning R-Squared (R^2) - performation of the regression model Logistic Regression Confusion matrix and accuracy score K Nearest Neighbor SVM Kernel SVM Kernel Tricks Non linear SVR (not SVM) Naive Bayes Naive Bayes Classifier intuition Decision Tree Classification Random Forest Classification K Means K means init trap K Means: choosing the number number of clusters K Mean Procedures Hierarchical Clustering Dendrograms Apriori Eclat Summary of UCB and Thompson Sampling Upper Confidence Bound Thompson Sampling Types of Natural Language Processing Classical v.s. deep learning models Bag of Words Intuition Assumptions Limitations choice of classifier using bag of words simplified training process null hypothesis and p-value Link to heading very useful post"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-01-10T13:02:28+00:00"><meta property="article:modified_time" content="2021-01-10T13:02:28+00:00"><meta property="article:tag" content="Cheat Sheet"><link rel=canonical href=https://joejztang.github.io/posts/machine-learning-az-cheat-sheet/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.4b392a85107b91dbdabc528edf014a6ab1a30cd44cafcd5325c8efe796794fca.css integrity="sha256-SzkqhRB7kdvavFKO3wFKarGjDNRMr81TJcjv55Z5T8o=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/img/favicon.svg sizes=any><link rel=icon type=image/png href=/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://joejztang.github.io/>130l
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://joejztang.github.io/posts/machine-learning-az-cheat-sheet/>machine learning a-z cheat sheet</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2021-01-10T13:02:28Z>January 10, 2021
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
13-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/machine-learning/>Machine Learning</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/cheat-sheet/>Cheat Sheet</a></span></div></div></header><div class=post-content><div><h1>Table of Contents</h1><nav id=TableOfContents><ul><li><a href=#null-hypothesis-and-p-value>null hypothesis and p-value</a></li></ul><ul><li><a href=#linear-regression>Linear Regression</a><ul><li><a href=#dummy-variable-trap-for-linear-regression>Dummy variable trap for linear regression</a></li><li><a href=#linear-regression-template>Linear Regression template</a></li><li><a href=#backward-elimination>Backward Elimination</a></li></ul></li><li><a href=#svr>SVR</a></li><li><a href=#decision-tree-regression>Decision Tree Regression</a></li><li><a href=#random-forest-and-ensemble-learning>Random Forest and Ensemble Learning</a></li><li><a href=#r-squared-r2---performation-of-the-regression-model>R-Squared (R^2) - performation of the regression model</a></li></ul><ul><li><a href=#logistic-regression>Logistic Regression</a><ul><li><a href=#confusion-matrix-and-accuracy-score>Confusion matrix and accuracy score</a></li></ul></li><li><a href=#k-nearest-neighbor>K Nearest Neighbor</a></li><li><a href=#svm>SVM</a><ul><li><a href=#kernel-svm>Kernel SVM</a></li><li><a href=#kernel-tricks>Kernel Tricks</a></li><li><a href=#non-linear-svr-not-svm>Non linear SVR (not SVM)</a></li></ul></li><li><a href=#naive-bayes>Naive Bayes</a><ul><li><a href=#naive-bayes-classifier-intuition>Naive Bayes Classifier intuition</a></li></ul></li><li><a href=#decision-tree-classification>Decision Tree Classification</a></li><li><a href=#random-forest-classification>Random Forest Classification</a></li></ul><ul><li><a href=#k-means>K Means</a><ul><li><a href=#k-means-init-trap>K means init trap</a></li><li><a href=#k-means-choosing-the-number-number-of-clusters>K Means: choosing the number number of clusters</a></li><li><a href=#k-mean-procedures>K Mean Procedures</a></li></ul></li><li><a href=#hierarchical-clustering>Hierarchical Clustering</a><ul><li><a href=#dendrograms>Dendrograms</a></li></ul></li></ul><ul><li><a href=#apriori>Apriori</a></li><li><a href=#eclat>Eclat</a></li></ul><ul><li><a href=#summary-of-ucb-and-thompson-sampling>Summary of UCB and Thompson Sampling</a></li><li><a href=#upper-confidence-bound>Upper Confidence Bound</a></li><li><a href=#thompson-sampling>Thompson Sampling</a></li></ul><ul><li><a href=#types-of-natural-language-processing>Types of Natural Language Processing</a></li><li><a href=#classical-vs-deep-learning-models>Classical v.s. deep learning models</a></li><li><a href=#bag-of-words>Bag of Words</a><ul><li><a href=#intuition>Intuition</a></li><li><a href=#assumptions>Assumptions</a></li><li><a href=#limitations>Limitations</a></li><li><a href=#choice-of-classifier-using-bag-of-words>choice of classifier using bag of words</a></li></ul></li></ul><ul><li><a href=#simplified-training-process>simplified training process</a></li></ul></nav></div><h2 id=null-hypothesis-and-p-value>null hypothesis and p-value
<a class=heading-link href=#null-hypothesis-and-p-value><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><a href=https://towardsdatascience.com/null-hypothesis-and-the-p-value-fdc129db6502 class=external-link target=_blank rel=noopener>very useful post</a></p><p>so to summarize somewhat into my own words:</p><ol><li>null hypothesis is</li></ol><blockquote><p>The null hypothesis, H0 is the commonly accepted fact; it is the opposite of the alternate hypothesis. Researchers work to reject, nullify or disprove the null hypothesis.</p></blockquote><p>this is coming from <a href=https://www.statisticshowto.com/probability-and-statistics/null-hypothesis/ class=external-link target=_blank rel=noopener>here</a></p><p>so how do we &ldquo;nullify&rdquo; the hypothesis? we are using p-value</p><ol start=2><li>we are using p-value to nullify the hypothesis</li></ol><p>the common chose p-value is 0.05, if the calculated p-value is less than 0.05, then it&rsquo;s nullified; otherwise, the hypothesis stays.</p><h1 id=regression---supervised-learninng>Regression - [Supervised Learninng]
<a class=heading-link href=#regression---supervised-learninng><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Regression is a useful tool to predict a continuous number.</p><h2 id=linear-regression>Linear Regression
<a class=heading-link href=#linear-regression><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Before building a linear regression model, there is a caveat</p><blockquote><p>Assumptions of Linear Regression:</p><ol><li>linearity</li><li><a href="https://www.google.com/search?q=homoscedasticity&amp;rlz=1C5CHFA_enUS651US651&amp;oq=homosec&amp;aqs=chrome.2.69i57j0i457j0i10i433j0j0i10l4.5514j0j9&amp;sourceid=chrome&amp;ie=UTF-8" class=external-link target=_blank rel=noopener>homoscedasticity</a></li><li>multivariate nomarlity</li><li>independence of errors</li><li>lack of <a href=https://en.wikipedia.org/wiki/Multicollinearity class=external-link target=_blank rel=noopener>multicollinearity</a></li></ol></blockquote><h3 id=dummy-variable-trap-for-linear-regression>Dummy variable trap for linear regression
<a class=heading-link href=#dummy-variable-trap-for-linear-regression><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>take an example here, say we have a model like this <code>y = b0 + b1*x1 + b2*x2 + b3*x3</code>, and we have one more column we want to take into consideration, say it&rsquo;s state.</p><p>we have two distinct states inside the column, they are &ldquo;New York&rdquo; and &ldquo;California&rdquo;. by encoding, we can say it it&rsquo;s NY, we will have a vector like <code>[1, 0]</code>, so for CA, we will have <code>[0, 1]</code>. just a representation.</p><p>so, it comes to an question that how should we include them, is it <code>y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1</code> or is it <code>y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1 + b5*D2</code>?</p><p>the answer is the first one <code>y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1</code>.</p><p>the reason is that, we know state is mutual exclusive, meaning it couldn&rsquo;t be both NY and CA, so naturally <code>D1+D2 = 1</code>. so if we introduce both D1 and D2 in the function we will violate &ldquo;lack of multicollinearity&rdquo; assumption.</p><h3 id=linear-regression-template>Linear Regression template
<a class=heading-link href=#linear-regression-template><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ol><li>import the libraries</li><li>import the dataset</li><li>encoding categorical data: hot-encoding will always come to the first columns</li><li>splitting the dataset into training set and test set</li><li>training the linear regression model on training set</li><li>predict the test dataset result</li></ol><h3 id=backward-elimination>Backward Elimination
<a class=heading-link href=#backward-elimination><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>the following code snippet copied from <a href=https://www.udemy.com/course/machinelearning/ class=external-link target=_blank rel=noopener>machine learnin a-z</a></p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>statsmodels.formula.api</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>sm</span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>backwardElimination</span>(x, sl):
</span></span><span style=display:flex><span>    numVars <span style=color:#ff7b72;font-weight:700>=</span> len(x[<span style=color:#a5d6ff>0</span>])
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>for</span> i <span style=color:#ff7b72;font-weight:700>in</span> range(<span style=color:#a5d6ff>0</span>, numVars):
</span></span><span style=display:flex><span>        regressor_OLS <span style=color:#ff7b72;font-weight:700>=</span> sm<span style=color:#ff7b72;font-weight:700>.</span>OLS(y, x)<span style=color:#ff7b72;font-weight:700>.</span>fit()
</span></span><span style=display:flex><span>        maxVar <span style=color:#ff7b72;font-weight:700>=</span> max(regressor_OLS<span style=color:#ff7b72;font-weight:700>.</span>pvalues)<span style=color:#ff7b72;font-weight:700>.</span>astype(float)
</span></span><span style=display:flex><span>        <span style=color:#ff7b72>if</span> maxVar <span style=color:#ff7b72;font-weight:700>&gt;</span> sl:
</span></span><span style=display:flex><span>            <span style=color:#ff7b72>for</span> j <span style=color:#ff7b72;font-weight:700>in</span> range(<span style=color:#a5d6ff>0</span>, numVars <span style=color:#ff7b72;font-weight:700>-</span> i):
</span></span><span style=display:flex><span>                <span style=color:#ff7b72>if</span> (regressor_OLS<span style=color:#ff7b72;font-weight:700>.</span>pvalues[j]<span style=color:#ff7b72;font-weight:700>.</span>astype(float) <span style=color:#ff7b72;font-weight:700>==</span> maxVar):
</span></span><span style=display:flex><span>                    x <span style=color:#ff7b72;font-weight:700>=</span> np<span style=color:#ff7b72;font-weight:700>.</span>delete(x, j, <span style=color:#a5d6ff>1</span>)
</span></span><span style=display:flex><span>    regressor_OLS<span style=color:#ff7b72;font-weight:700>.</span>summary()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> x
</span></span></code></pre></div><h2 id=svr>SVR
<a class=heading-link href=#svr><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>SVR vs SVM, both have support vector in the names, but they are for different purposes.</p><p>SVR stands for support vector regression, in essence it&rsquo;s a regression algorithm, SVR will works forr continuous values</p><p>whereas SVM is a classification algorithm.</p><p>As name indicated, SVR could be used to predict.</p><p>template for SVR:</p><ol><li>import libs</li><li>import dataset</li><li>feature scaling</li><li>train the SVR model on the whole dataset</li><li>predict new result</li><li>visualize the result</li></ol><p>Noted number 3. why do a feature scaling? Because in machine learning algorithms, it&rsquo;s essential to calculate <em>distance between data</em>.
If not scale, the feature with higher value range starts dominating.
check details <a href=https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35 class=external-link target=_blank rel=noopener>here</a></p><p>And remember to call <code>inverse_transfrom</code> method after you got the predictions. Make sure to use the right scaler.</p><h2 id=decision-tree-regression>Decision Tree Regression
<a class=heading-link href=#decision-tree-regression><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Decision tree can be used to both regression and classification.</p><p>The essence of decision tree regression is to split the tree, calculate the entropy for each leaf node in order to find the most important attribute.</p><p>So, that said, we don&rsquo;t need to do feature scaling on decision tree model.</p><p>Noted that decision tree regression is not suitable for single feature model(?), it will perform better for multi feature model.</p><p>The same for random forest regression, no feature scaling</p><h2 id=random-forest-and-ensemble-learning>Random Forest and Ensemble Learning
<a class=heading-link href=#random-forest-and-ensemble-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Random forest is one kind of ensemble learning. The essence of ensemble learning, random forest specifically in our case, is to enlarge the hypothesis, and combine hypotheses later on.</p><p>Random forest seems more adaptable on classification because by utilizing the essence of ensemble learning, we hope it will decrease the classification: it will be unlikely for all 5 hypotheses to misclassify than one; in other word, we are more safe to trust the all 5 &ldquo;misclassifies&rdquo;</p><h2 id=r-squared-r2---performation-of-the-regression-model>R-Squared (R^2) - performation of the regression model
<a class=heading-link href=#r-squared-r2---performation-of-the-regression-model><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Simple definition of R squared.</p><p><code>SS res = SUM(yi - y^i)^2</code>
<code>SS tot = SUM(yi - yavg)^2</code></p><p><code>R^2 = 1-SS res/SS tot</code></p><p>R^2 ideally would be from 0 to 1, the more to 1 the better, the more to 1 the more fit to the data.</p><p><em>Adjusted R Squared</em></p><p>The intuition that we have an adjusted R squared is that because it (original R squared) will never decrease if add more feature into the hypothesis</p><p><code>Adj R^2 = 1-(1-R^2)(n-1)/(n-p-1_)</code></p><p>where <code>p - number of regressors</code> and <code>n - sample size</code></p><p>R squared is also a criteria to choose which model to use.</p><h1 id=classification---unsupervised-learning>Classification - [Unsupervised Learning]
<a class=heading-link href=#classification---unsupervised-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Classification is a powerful tool to predict a category.</p><h2 id=logistic-regression>Logistic Regression
<a class=heading-link href=#logistic-regression><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Logistic regression template</p><ol><li>import libs</li><li>import dataset</li><li>splitting the dataset into training and test set</li><li>feature scaling</li><li>training (fit) the logistic regression model on training dataset</li><li>predict a new result</li><li>predicting the test set result</li><li>making the confusion matrix</li><li>visualize both train and test set results</li></ol><p>1-4 are data preprocessing, 5 is fit step, 6,7 are prediciton and rest are visualization.</p><h3 id=confusion-matrix-and-accuracy-score>Confusion matrix and accuracy score
<a class=heading-link href=#confusion-matrix-and-accuracy-score><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>In <code>sklearn.metrics</code>, but what is underneath?</p><blockquote><p>A confusion matrix is a table that is often used to describe the performance of a classification model (or &ldquo;classifier&rdquo;) on a set of test data for which the true values are known.</p></blockquote><p>please go to <a href=https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology class=external-link target=_blank rel=noopener>dataschool</a></p><p><img src=https://www.dataschool.io/content/images/2015/01/confusion_matrix_simple2.png alt=confustion_matrix></p><h2 id=k-nearest-neighbor>K Nearest Neighbor
<a class=heading-link href=#k-nearest-neighbor><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h2 id=svm>SVM
<a class=heading-link href=#svm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>back to SVR, in stead of using it to predict continous numbers, we use it to classify.</p><p>Some terms:</p><ul><li>hyperplane</li><li>support vectors</li><li>maximum margin</li></ul><p>SVM is somewhat special because the support vectors it uses to construct margin are the &ldquo;edge cases&rdquo;. Poeple find it predicts better than non SVM algo often because it uses the edge cases.</p><h3 id=kernel-svm>Kernel SVM
<a class=heading-link href=#kernel-svm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>If the data is linearly separable, then use linear SVM; otherwise, we cannot use linear SVM.</p><p>Then the question becomes what to use?</p><p>Some intuition that classify some non linear dataset using SVM is to make dataset to a higher dimension.</p><p>The pros of doing that is it sometimes is easy to find the hyperplane when in a higher dimension; the cons is computing intensive.</p><p>After using SVM solving it in a higher dimension, then map it back.</p><p>Now, after the intuition, how do we actually do it?</p><p>Since we are under Kernel Function, yes, by applying kernel functions, basically we are doing those.</p><p>An <a href=https://medium.com/@suvigya2001/the-gaussian-rbf-kernel-in-non-linear-svm-2fb1c822aae0 class=external-link target=_blank rel=noopener>example</a> here. In the middle of the post, it generates a nice graph. First using kernel function in this case RBF pull data from 2d to 3d, classify them, and then push back data from 3d to 2d. Contour will be left on 2d plate. (kernel trick???)</p><p>Some common kernel functions that we might use including:</p><ul><li>Gaussian RBF</li><li>Polynomial kernel function</li><li>Sigmoid kernel function</li></ul><h3 id=kernel-tricks>Kernel Tricks
<a class=heading-link href=#kernel-tricks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>According to this <a href=https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d class=external-link target=_blank rel=noopener>post</a>, keneral trick is so important that it bridges linearity to non-linearity.</p><h3 id=non-linear-svr-not-svm>Non linear SVR (not SVM)
<a class=heading-link href=#non-linear-svr-not-svm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>By using kernel trick, or pull data into higher demensions-find max and min hyperplane-push higher demension back to original, we could find a tube (in this time it&rsquo;s a curved cube instead of two lines)</p><p>How do we do that? Kernel tricks. So kernel tricks is very important.</p><h2 id=naive-bayes>Naive Bayes
<a class=heading-link href=#naive-bayes><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The following is the Bayes Thereom</p><p>\( {P(A|B)}{P(B)} = {P(B|A)}{P(A)}\)</p><h3 id=naive-bayes-classifier-intuition>Naive Bayes Classifier intuition
<a class=heading-link href=#naive-bayes-classifier-intuition><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Plan to attack a problem, have to borrow Bayes Thereom again</p><p>\( {P(A|B)} = \frac{P(B|A)*P(A)}{P(B)} \)</p><p>We are now giving each element a new name:</p><ol><li>\( {P(A)} \), the Prior Probablility</li><li>\( {P(B)} \), the Marginal Likelihood</li><li>\( {P(B|A)} \), the Likelihood</li><li>\( {P(A|B)} \), the Posterior Probablility</li></ol><p>When attack a problem, we want to calculate elements <em>in order</em>.</p><p>A concrete example, we assign <code>B</code> as the features dataset <code>X</code>, and given <code>X</code>, we have multiple categories that can classify to; in other words, <code>A</code> can assign to <code>cat1</code>, <code>cat2</code>, etc.</p><p>So the function now becomes:</p><p>\( {P(cat1|X)} = \frac{P(X|cat1)*P(cat1)}{P(X)} \)</p><p>If we want to predict a data point, <code>x</code>, we need to calculate all \( {P(cat1|X)} \) and \( {P(cat2|X)} \), at last compare them and assign to the correct cat</p><p>Noted for number2, the marginal likelihood. Calculating that likelihood will introduce a &ldquo;margin&rdquo;, which will be having the similar features of the &ldquo;newly added point/the point to be predicted&rdquo;. So feels like choosing that &ldquo;margin&rdquo; is an critical work during the calulation.</p><h2 id=decision-tree-classification>Decision Tree Classification
<a class=heading-link href=#decision-tree-classification><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Classification and Regression Trees (CART)</p><p>Decision Tree recently was dying, but some other algorithm utilize the simplicity of the decision trees to make it reborn.</p><p>They are Random Forest, Gradient Boosting, etc.</p><h2 id=random-forest-classification>Random Forest Classification
<a class=heading-link href=#random-forest-classification><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Random Forest is some kind of ensemble learning.</p><h1 id=evaluation-of-classification>Evaluation of classification
<a class=heading-link href=#evaluation-of-classification><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><ul><li>confusion matrix: In general, false positive is more like a warning, whereas false negative is like the real &ldquo;error&rdquo;.
In diagnal, they are the right prediction.</li><li>Cumulative Accuracy Profile (CAP) vs. Receiver Operating Characteristic (ROC)<ul><li><a href="https://community.tibco.com/wiki/gains-vs-roc-curves-do-you-understand-difference?exp=cloud&amp;pref=off" class=external-link target=_blank rel=noopener>some explains</a></li><li>when doing cap analysis, this guy introduce AR <img src=/images/ar.png alt=ar> and some criteria <img src=/images/cap_analysis.png alt=cri></li></ul></li></ul><h1 id=clustering>Clustering
<a class=heading-link href=#clustering><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><h2 id=k-means>K Means
<a class=heading-link href=#k-means><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ol><li>choose the number K of clusters</li><li>select at random K points. the centroids (not necessarily from your dataset)</li><li>assign each data point to the closest centroid, that forms k clusters (calculate distance to the centroid)</li><li>compute and place the new centroid of each cluster. (average of data within the cluster)</li><li>reassign each data point to the new closest centroid.</li></ol><p>if any <strong>reassignment</strong> took place, go to 4 again.</p><p>otherwise, the alg is finished.</p><h3 id=k-means-init-trap>K means init trap
<a class=heading-link href=#k-means-init-trap><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>different select of centroids will result in false consequence.</p><p>use <strong>Kmean++</strong> algorithm.</p><h3 id=k-means-choosing-the-number-number-of-clusters>K Means: choosing the number number of clusters
<a class=heading-link href=#k-means-choosing-the-number-number-of-clusters><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>\( WCSS = \sum_{Pi in Cluster j}^{total i} \sum_{j}^{total j} distance(Pi, Cj)^2 \)</p><p>WCSS will be decreasing continously and to 0 if every point has a centroid of its own.</p><p>What should we do to optimize it? Draw WCSS vs Number of Clusters and using elbow method to decide the number of clusters.</p><h3 id=k-mean-procedures>K Mean Procedures
<a class=heading-link href=#k-mean-procedures><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ol><li>import libs</li><li>import datasets</li><li>elbow method to find number of clusters</li><li>train the K Means model on the dataset</li><li>visualization</li></ol><h2 id=hierarchical-clustering>Hierarchical Clustering
<a class=heading-link href=#hierarchical-clustering><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We majorly focus on the agglomerative clustering rather than divisive here</p><ol><li>Make each data point a single-point cluster -> forms N clusters</li><li>take the two closest clusters and make them one cluster</li><li>repeat until there is only one cluster</li></ol><p>How to define the distance between the two clusters?</p><p>There are several options: 1. closest points, 2. furthest points, 3. average distance, 4. distance between centoids.
As long as it is consistent, then it&rsquo;s fine</p><p>How to decide the best cluster numbers?</p><h3 id=dendrograms>Dendrograms
<a class=heading-link href=#dendrograms><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p><a href=https://medium.com/@displayr/what-is-a-dendrogram-9de4fc1f131e class=external-link target=_blank rel=noopener>what is a dendrogram</a></p><p>And the quickest way making a decision is to see which vertical line between the two horizontal line is the longest.</p><p>like this example below</p><p><img src=/images/dendrogram.png alt=dendrogram></p><p>we see the correct answer for this is 3 clusters, not 2 clusters because the red line has the largest distance.</p><h1 id=association-rule-learning>Association Rule Learning
<a class=heading-link href=#association-rule-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><h2 id=apriori>Apriori
<a class=heading-link href=#apriori><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>\( {lift(M_1 -> M_2)} = \frac{confidence(M_1 -> M_2)}{support(M_2)} \)</p><p>A practical usage of this alg would be frequent purchased together. (I am guessings)</p><h2 id=eclat>Eclat
<a class=heading-link href=#eclat><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Only <code>support</code> matters. Still use <code>apriori</code> model to give rules, but using eclat method to analyze.</p><h1 id=reinforcement-learning>Reinforcement Learning
<a class=heading-link href=#reinforcement-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>What is reinforcement learning?</p><blockquote><p>Reinforcement Learning is a subset of machine learning. It enables an agent to learn through the consequences of actions in a specific environment. It can be used to teach a robot new tricks, for example.
&ndash; towardsdatascience.com</p></blockquote><h2 id=summary-of-ucb-and-thompson-sampling>Summary of UCB and Thompson Sampling
<a class=heading-link href=#summary-of-ucb-and-thompson-sampling><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>UCB is a deterministic algorithm whereas Thompson Sampling is a probalistic algorithm.</li></ul><h2 id=upper-confidence-bound>Upper Confidence Bound
<a class=heading-link href=#upper-confidence-bound><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Coming from <a href=https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html class=external-link target=_blank rel=noopener>multi-armed bandit</a> problem.</p><p>Some simplified steps for UCB alg:</p><ol><li>At each round n, we consider two numbers for each ad i:<ul><li>\( N_i(n) \) - the number of times the ad i was selected up to round n</li><li>\( R_i(n) \) - the sum of rewards of the ad i up to round n</li></ul></li><li>From these two numbers we compute:<ul><li>the average reward of ad i up to round n \( \bar{r}_i(n) = \frac{R_i(n)}{N_i(n)} \)</li><li>the confidence interval \( [\bar{r}_i(n) - \Delta_i(n), \bar{r}_i(n) + \Delta_i(n)] \) at round n with \( \Delta_i(n) = \sqrt{\frac{3log(n)}{2N_i(n)}} \)</li></ul></li><li>We select the ad i that has the maximum UCB \( \bar{r}_i(n) + \Delta_i(n) \)</li></ol><h2 id=thompson-sampling>Thompson Sampling
<a class=heading-link href=#thompson-sampling><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>some simplified steps for Thompson Sampling</p><ol><li>At each round n, we consider two numbers for each ad i:<ul><li>\( N_i^1(n) \) - the number of times the ad i got reward 1 up to round n.</li><li>\( N_i^0(n) \) - the number of times the ad i got reward 0 up to round n.</li></ul></li><li>For each ad i, we take a random draw from the distribution \( \theta_i(n) = \beta(N_i^1(n)+1, N_i^0(n)+1) \)</li><li>We select the ad that has the highest \( \theta_i(n) \)</li></ol><h1 id=nlp>NLP
<a class=heading-link href=#nlp><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><h2 id=types-of-natural-language-processing>Types of Natural Language Processing
<a class=heading-link href=#types-of-natural-language-processing><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Once there were classic NLP, and there were deep learning.</p><p>Now we have the intersection called DNLP, within that intersection, we have seq2seq</p><p>TLDR: classic NLP DNLP(including seq2seq) Deep Learning</p><h2 id=classical-vs-deep-learning-models>Classical v.s. deep learning models
<a class=heading-link href=#classical-vs-deep-learning-models><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>classical examples:<ul><li>if/else rules used to be used on chatbot</li><li>some audio frequency analysis</li><li>bag of words used for classification</li></ul></li><li>not so classical &ndash;> deep learning<ul><li>CNN for text recognition used for classification</li><li>seq2seq can be used for many applications.</li></ul></li></ul><h2 id=bag-of-words>Bag of Words
<a class=heading-link href=#bag-of-words><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><blockquote><p>The bag-of-words model is a simplifying <strong>representation</strong> used in natural language processing and information retrieval (IR).
&ndash; wikipedia</p></blockquote><h3 id=intuition>Intuition
<a class=heading-link href=#intuition><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>One specific sentence is represented by a array <code>[0, 0, ..., 0]</code>, where <code>array[0]</code> represents the start of the sentence (SOS), <code>array[1]</code> represents the end of the sentence (EOS), <code>array[-1]</code> represents the special words.</p><p>Usually array is with length of 20000 elements long.</p><p>The number of 20000 is because of</p><blockquote><p>Most adult native test-takers knows ranging from 20000-35000
&mdash; The Economist</p></blockquote><p>By feeding training data, some sentences prepared are used to train the model.</p><h3 id=assumptions>Assumptions
<a class=heading-link href=#assumptions><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li>position of the words in the document doesn&rsquo;t matter.</li></ul><h3 id=limitations>Limitations
<a class=heading-link href=#limitations><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Limitations on bag of words algorithm is that it can only answer Y/N question.</p><h3 id=choice-of-classifier-using-bag-of-words>choice of classifier using bag of words
<a class=heading-link href=#choice-of-classifier-using-bag-of-words><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>see from <a href=https://theflyingmantis.medium.com/text-classification-in-nlp-naive-bayes-a606bf419f8c class=external-link target=_blank rel=noopener>here</a></p><ul><li>No data &ndash;> handwritten rules.</li><li>less training data &ndash;> naive bayes</li><li>reasonable amount &ndash;> SVM & Logical regression. Decision trees maybe.</li></ul><h1 id=neural-network>Neural Network
<a class=heading-link href=#neural-network><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><h2 id=simplified-training-process>simplified training process
<a class=heading-link href=#simplified-training-process><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ol><li>Randomly initialize the weights to small numbers close to 0 (but not 0)</li><li>input the first observation of your dataset in the input layer, each feature in one input node.</li><li>forward-propagation, getting result y.</li><li>compare the predicted result to the actual result. measure the generated error.</li><li>back propagation. <strong>update the weights according to how much they are responsible for the error.</strong> the learning rate decides by how much we update the weights.</li><li>repeat 1-5 update the weights after each observation (<strong>Reinforcement Learning</strong>). if update the weigths only after a batch of observations (<strong>Batch Learning</strong>).</li><li>When the whole traning set passed thru the ANN, that makes an <strong>epoch</strong>. Redo more epochs.</li></ol></div><footer><div class=comments><script>let getTheme=window.localStorage&&window.localStorage.getItem("colorscheme"),themeInParams="github-light";getTheme==null&&(themeInParams!==""&&themeInParams!=="auto"?getTheme=themeInParams:getTheme=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");let theme=getTheme==="dark"?"github-dark":"github-light",s=document.createElement("script");s.src="https://utteranc.es/client.js",s.setAttribute("repo","joejztang/joejztang.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("theme",theme),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>Â©
1990 -
2025
130l
Â·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-YNPYVVT1S7"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YNPYVVT1S7")}</script></body></html>