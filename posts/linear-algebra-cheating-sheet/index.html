<!doctype html><html lang=en><head><title>linear algebra cheating sheet · 130l</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="130l"><meta name=description content="This cheating sheet is only computer/Machine Learning/Deep Learning wise.
I have read Paul Dawkin&rsquo;s previous contents and accumulated some gists that I found useful. (content seems not to be there anymore, for more infomation, please contact the author)"><meta name=keywords content="blog,science,technology,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="linear algebra cheating sheet"><meta name=twitter:description content="This cheating sheet is only computer/Machine Learning/Deep Learning wise.
I have read Paul Dawkin’s previous contents and accumulated some gists that I found useful. (content seems not to be there anymore, for more infomation, please contact the author)"><meta property="og:url" content="http://www.example.com/posts/linear-algebra-cheating-sheet/"><meta property="og:site_name" content="130l"><meta property="og:title" content="linear algebra cheating sheet"><meta property="og:description" content="This cheating sheet is only computer/Machine Learning/Deep Learning wise.
I have read Paul Dawkin’s previous contents and accumulated some gists that I found useful. (content seems not to be there anymore, for more infomation, please contact the author)"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-03-22T11:07:57+00:00"><meta property="article:modified_time" content="2018-03-22T11:07:57+00:00"><meta property="article:tag" content="Cheating Sheet"><link rel=canonical href=http://www.example.com/posts/linear-algebra-cheating-sheet/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.4b392a85107b91dbdabc528edf014a6ab1a30cd44cafcd5325c8efe796794fca.css integrity="sha256-SzkqhRB7kdvavFKO3wFKarGjDNRMr81TJcjv55Z5T8o=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/img/favicon.svg sizes=any><link rel=icon type=image/png href=/img/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/img/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=http://www.example.com/>130l
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Posts</a></li><li class=navigation-item><a class=navigation-link href=/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=http://www.example.com/posts/linear-algebra-cheating-sheet/>linear algebra cheating sheet</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2018-03-22T11:07:57Z>March 22, 2018
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
16-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/math/>Math</a>
<span class=separator>•</span>
<a href=/categories/linear-algebra/>Linear Algebra</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/cheating-sheet/>Cheating Sheet</a></span></div></div></header><div class=post-content><p>This cheating sheet is only computer/Machine Learning/Deep Learning wise.</p><p>I have read <a href=http://tutorial.math.lamar.edu class=external-link target=_blank rel=noopener>Paul Dawkin</a>&rsquo;s previous contents and accumulated some gists that I found useful. (content seems not to be there anymore, for more infomation, please contact the author)</p><h2 id=inverse-matrices-and-elementary-matrices>inverse matrices and elementary matrices
<a class=heading-link href=#inverse-matrices-and-elementary-matrices><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li><p>If \(A\) is a square matrix and we can find another matrix of the same size, say \(B\), such that</p><p>\[ AB = BA = I \]</p><p>then we call \(A\) invertible and we say that \(B\) is an inverse of the matrix \(A\). If we can’t find such a matrix \(B\) we call A a singular matrix.</p></li><li><p>Suppose that \(A\) and \(B\) are invertible matrices of the same size. Then,</p><p><strong>(a)</strong> \(AB\) is invertible and \( {(AB)}^{−1} = {B}^{−1}{A}^{−1} \).</p><p><strong>(b)</strong> \({A}^{−1}\) is invertible and \( {({A}^{−1})}^{-1} =A\).</p><p><strong>(c)</strong> For \( n=0,1,2,&mldr; {A}^n \) is invertible and \( {({A}^n)}^{-1} = {A}^{−n} ={(A^{−1})}^n \).</p><p><strong>(d)</strong> If \(c\) is any non-zero scalar then \(cA\) is invertible and \( {(cA)}^{−1} = {1 \over c} {A}^{−1} \)
<strong>(e)</strong> \(A^T\) is invertible and \( {(A^T)}^{-1} = {(A^{−1})}^T \).</p></li></ul><h3 id=elementary-matrix>elementary matrix
<a class=heading-link href=#elementary-matrix><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li>A square matrix is called an <strong>elementary matrix</strong> if it can be obtained by applying a single elementary row operation to the identity matrix of the same size.</li></ul><h2 id=finding-inverse-matrices>finding inverse matrices
<a class=heading-link href=#finding-inverse-matrices><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li><p>If \(A\) is an \(n \times n\) matrix then the following statements are equivalent.</p><p><strong>(a)</strong> \(A\) is invertible.</p><p><strong>(b)</strong> The only solution to the system \(A \mathbf{x} = 0\) is the trivial solution.</p><p><strong>(c)</strong> \(A\) is row equivalent to \(I_n\).</p><p><strong>(d)</strong> \(A\) is expressible as a product of elementary matrices.</p><p><strong>(e)</strong> \(A \mathbf{x} = \mathbf{b} \) has exactly one solution for every \(n \times 1\) matrix \(\mathbf{b}\).</p><p><strong>(f)</strong> \(A \mathbf{x} = \mathbf{b} \) is consistent for every \(n \times 1\) matrix \(\mathbf{b}\).</p><p><strong>(g)</strong> \( det(A) \ne 0 \).</p></li></ul><h3 id=consistent-and-inconsistent>consistent and inconsistent
<a class=heading-link href=#consistent-and-inconsistent><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>A system of linear equations is called <strong>inconsistent</strong> if it has no solutions. A system which has a solution is called <strong>consistent</strong>.</p><h2 id=special-matrices>Special Matrices
<a class=heading-link href=#special-matrices><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li><p>About triangular matrix</p><p><strong>(a)</strong> The product of lower triangular matrices will be a lower triangular matrix.</p><p><strong>(b)</strong> The product of upper triangular matrices will be an upper triangular matrix.</p><p><strong>(c)</strong> The inverse of an invertible lower triangular matrix will be a lower triangular matrix.</p><p><strong>(d)</strong> The inverse of an invertible upper triangular matrix will be an upper triangular matrix.</p></li><li><p>About symmetric matrix</p><p><strong>(a)</strong> For any matrix \(A\) both \(AA^T\) and \(A^T A\) are symmetric.</p><p><strong>(b)</strong> If \(A\) is an invertible symmetric matrix then \(A^{−1}\) is symmetric.</p><p><strong>(c)</strong> If \(A\) is invertible then \(AA^T\) and \(A^T A\) are both invertible.</p></li></ul><h2 id=determinants>Determinants
<a class=heading-link href=#determinants><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li><p>Let \(A\) be an \(n \times n\) matrix and \(c\) be a scalar then,
\[det(cA)=c^n det(A)\]</p></li><li><p>If \(A\) and \(B\) are matrices of the same size then
\[ det(AB)=det(A)det(B) \]</p></li><li><p>Suppose that \(A\) is an invertible matrix then,
\[ det(A^{−1})= {1 \over det(A)} \]</p></li></ul><h3 id=singular-and-non-singular>singular and non-singular
<a class=heading-link href=#singular-and-non-singular><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>A square matrix A is invertible if and only if \( det (A) \ne 0 \) . A matrix that is
invertible is often called <strong>non-singular</strong> and a matrix that is not invertible is often called <strong>singular</strong>.</p></li><li><p>If A is a square matrix then, \( det(A)=det(A^T) \).</p></li><li><p>Suppose that \(A\) is an \(n \times n\) triangular matrix then, \( det(A)= a11\ a22\ &mldr;\ ann \).</p></li></ul><h2 id=adjoint>adjoint
<a class=heading-link href=#adjoint><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=matrix-of-cofactors-from-a-adjoint-of-a>matrix of cofactors from A, Adjoint of A
<a class=heading-link href=#matrix-of-cofactors-from-a-adjoint-of-a><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>If \(A\) is a square matrix then the minor of \(a_{ij}\),</p><p>denoted by \(M_{ij}\)</p><p>is the determinant of the submatrix that results from removing the \(i^{th}\) row and \(j^{th}\) column of \(A\). If \(A\) is a square matrix then the <strong>cofactor of \(a_{ij}\)</strong></p><p>denoted by \(C_{ij}\)</p><p>is the number \((−1)^{i+j} M_{ij}\) .</p><ul><li>Let \(A\) be an n×n matrix and \(C_{ij}\)</li></ul><p>be the cofactor of \(a_{ij}\). The <strong>matrix of cofactors from A</strong> is,</p><p>\begin{bmatrix}
C11 & C12 & &mldr; & C1n\\
C21 & C22 & &mldr; & C2n\\
\vdots & \vdots & \ddots & \vdots\\
Cn1 & Cn2 & &mldr; & Cnn
\end{bmatrix}</p><p>The <strong>adjoint of A</strong> is the transpose of the matrix of cofactors and is denoted by adj(A).</p><ul><li><p>If A is an invertible matrix then
\[ A^{−1} = {1 \over det(A)}adj(A) \]</p></li><li><p>Let A be a square matrix.</p><p><strong>(a)</strong> If \(B\) is the matrix that results from multiplying a row or column of \(A\) by a
scalar, \(c\), then \( det(B)= cdet(A)\)</p><p><strong>(b)</strong> If \(B\) is the matrix that results from interchanging two rows or two columns of
\(A\) then \(det(B)=−det(A)\)</p><p><strong>(c)</strong> If \(B\) is the matrix that results from adding a multiple of one row of \(A\) onto another row of \(A\) or adding a multiple of one column of \(A\) onto another column of \(A\) then \(det(B)=det(A)\)</p></li></ul><h2 id=cramers-rule>Cramer’s Rule
<a class=heading-link href=#cramers-rule><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Suppose that \(A\) is an \(n \times n\) invertible matrix. Then the solution to the system
\(Ax=b\) is given by,</p><p>\[ x_1 = { det(A_1) \over det(A) }, x_2 = { det(A_2) \over det(A) }, &mldr;, x_n = { det(A_n) \over det(A) } \]</p><p>where \(A_i\) is the matrix found by replacing the \(i^{th}\) column of \(A\) with \(\mathbf{b}\).</p><h2 id=euclidean-n-space>Euclidean n-space
<a class=heading-link href=#euclidean-n-space><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li><p>Two non-zero vectors, <strong>u</strong> and <strong>v</strong>, are orthogonal if and only if \({u} \centerdot {v} = 0 \)</p></li><li><p>Suppose that \(u\) and \(a \ne 0\) are both vectors in 2-space or 3-space then,</p></li></ul><p>\[proj_a \mathbf{u} = { {\mathbf{u} \centerdot \mathbf{a} } \over {\parallel \mathbf{a} \parallel}^2 } \mathbf{a}\]</p><p>and the vector component of <strong>u</strong> orthogonal to <strong>a</strong> is given by,</p><p>\[\mathbf{u} - proj_a \mathbf{u} = {\mathbf{u}- {\mathbf{u} \centerdot \mathbf{a} } \over {\parallel \mathbf{a} \parallel}^2 } \mathbf{a}\]</p><ul><li><p>If u and v are two vectors in 3-space then the cross product, denoted by u × v and is defined in one of three ways.</p><p><strong>(a)</strong> \( \mathbf{u} \times \mathbf{v} = (u_2v_3 −u_3v_2,u_3v_1 −u_1v_3,u_1v_2 −u_2v_1) \)-Vector Notation.</p><p><strong>(b)</strong> \( \mathbf{u} \times \mathbf{v} = (\begin{vmatrix} u_2 & u_3 \\ v_2 & v_3\end{vmatrix}, -\begin{vmatrix} u_1 & u_3 \\ v_1 & v_3\end{vmatrix}, \begin{vmatrix} u_1 & u_2 \\ v_1 & v_2\end{vmatrix}) \) - Using 2 x 2 determinants</p><p><strong>(c)</strong> \( \mathbf{u} \times \mathbf{v} = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k}\\ u_1 & u_2 & u_3\\ v_1 & v_2 & v_3\end{vmatrix} \) - Using 3 x 3 determinants</p></li><li><p>The cross product <strong>a × b</strong> is defined as a vector <strong>c</strong> that is perpendicular (orthogonal) to both a and <strong>b</strong>, with a direction given by the right-hand rule and a magnitude equal to the area of the parallelogram that the vectors span.</p></li><li><p>Suppose \(\mathbf{u}, \mathbf{v}\), and \(\mathbf{w}\) are vectors in 3-space and \(c\) is any scalar then</p><p><strong>(a)</strong> \( \mathbf{u×v=−(v×u)} \)</p><p><strong>(b)</strong> \( \mathbf{u×(v+w)=(u×v)+(u×w)} \)</p><p><strong>(c)</strong> \( \mathbf{(u+v)×w=(u×w)+(v×w)} \)</p><p><strong>(d)</strong> \( c\mathbf{(u×v)}=(c\mathbf{u})×\mathbf{v}=\mathbf{u}×(c\mathbf{v}) \)</p><p><strong>(e)</strong> \( \mathbf{u×0=0×u=0} \)</p><p><strong>(f)</strong> \( \mathbf{u×u=0} \)</p></li></ul><h3 id=lagrange-identity>lagrange identity
<a class=heading-link href=#lagrange-identity><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose \( \mathbf{u, v} \), and \( \mathbf{w} \) are vectors in 3-space then,</p><p><strong>(a)</strong> \( \mathbf{u \centerdot (u×v)=0} \)</p><p><strong>(b)</strong> \( \mathbf{v \centerdot (u×v)=0} \)</p><p><strong>(c)</strong> \( {\parallel \mathbf{u \times v} \parallel}^2 = {\parallel \mathbf{u} \parallel}^2\ {\parallel \mathbf{v} \parallel}^2 −{\mathbf{(u \centerdot v)}}^2 \) - This is called <strong>Lagrange’s Identity</strong></p><p><strong>(d)</strong> \( \mathbf{ u \times (v \times w)=(u \centerdot w)v−(u \centerdot v)w } \)</p><p><strong>(e)</strong> \( \mathbf{(u \times v) \times w=(u \centerdot w)v−(v \centerdot w)u} \)</p></li><li><p>Suppose that \(\mathbf{u}\) and \(\mathbf{v}\) are vectors in 3-space and let θ be the angle between them then,</p></li></ul><p>\[ \parallel \mathbf{u \times v} \parallel = \parallel \mathbf{u} \parallel\parallel \mathbf{v} \parallel \sin\theta \]</p><h3 id=euclidean-inner-product>euclidean inner product
<a class=heading-link href=#euclidean-inner-product><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li>Suppose \( \mathbf{u}=(u_1,u_2,&mldr;,u_n) \) and \( \mathbf{v} = (v_1,v_2,&mldr;,v_n) \) are two vectors in \( \mathbb{R}^n \) then the <strong>Euclidean inner product</strong> denoted by \( \mathbf{ u \centerdot v } \) is defined to be \( \mathbf{u \centerdot v} = u_1v_1 +u_2v_2 +&mldr;+u_nv_n \)</li></ul><h3 id=euclidean-norm>euclidean norm
<a class=heading-link href=#euclidean-norm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li>Suppose \( \mathbf{u} = (u_1,u_2,&mldr;,u_n) \) is a vector in \( \mathbb{R}^n \) then the <strong>Euclidean norm</strong> is, \[ \parallel \mathbf{u} \parallel = (\mathbf{u \centerdot u})^{1 \over 2} = \sqrt{ u_1^2 + u_2^2 + &mldr; + u_n^2 } \]</li></ul><h3 id=euclidean-distance>euclidean distance
<a class=heading-link href=#euclidean-distance><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose \( \mathbf{u}=(u_1,u_2,&mldr;,u_n) \)and \( \mathbf{v} =(v_1,v_2,&mldr;,v_n) \) are two points in \(\mathbb{R}^n\) then the <strong>Euclidean distance</strong> between them is defined to be, \[ d(\mathbf{u}, \mathbf{v}) = \parallel \mathbf{u-v} \parallel = \sqrt{(u_1-v_1)^2+(u_2-v_2)^2+&mldr;+(u_n-v_n)^2} \]</p></li><li><p>If \(\mathbf{u}\) and \(\mathbf{v}\) are two vectors in \(\mathbb{R}^n\) then, \[ \mathbf{u \centerdot v} = {1 \over 4} {\parallel \mathbf{u+v} \parallel}^2 - {1 \over 4} {\parallel \mathbf{u-v} \parallel}^2 \]</p></li><li><p>\( f: \mathbb{R}^n \rightarrow \mathbb{R}^m \) for example define \( T: \mathbb{R}^2 \rightarrow \mathbb{R}^4\) as, \[ T(x_1, x_2) = (3x_1-4x_2, x_1+2x_2, 6x_1-x_2, 10x_2) = \begin{bmatrix} 3 & -4 \\ 1 & 2 \\ 6 & -1 \\ 0 & 10 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2\end{bmatrix} \Rightarrow \mathbf{w} = A\mathbf{x} \]</p></li></ul><h3 id=linear-transformation>linear transformation
<a class=heading-link href=#linear-transformation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li>A function \( T: \mathbb{R}^n \rightarrow \mathbb{R}^m \) is called a <strong>linear transformation</strong> if for all \(\mathbf{u}\) and \(\mathbf{v}\) in \(\mathbb{R}^n\) and all scalars \(c\) we have, \[ T(\mathbf{u+v}) = T(\mathbf{u}) + T(\mathbf{v}) \] \[ T(c\mathbf{u}) = cT(\mathbf{u}) \]</li></ul><h3 id=induced-transformation>induced transformation
<a class=heading-link href=#induced-transformation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li>If A is an \(m \times n\) matrix then its <strong>induced transformation</strong>, \( T_A: \mathbb{R}^n \rightarrow \mathbb{R}^m \) defined as, \[ T_A(\mathbf{x}) = A\mathbf{x} \] is a linear transformation.</li></ul><h3 id=matrix-induced-by-t>matrix induced by T
<a class=heading-link href=#matrix-induced-by-t><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li>Let \( T: \mathbb{R}^n \rightarrow \mathbb{R}^m \) be a linear transformation, then there is an \(m \times n\) matrix such that \(T = T_A\) (recall that \(T_A\) is the transformation induced by \(A\)). The matrix \(A\) is called the <strong>matrix induced by \(T\)</strong> and is sometimes denoted as \(A = [T]\).</li></ul><h2 id=example-of-linear-transformations>example of linear transformations
<a class=heading-link href=#example-of-linear-transformations><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>please refer to Paul Dawkins <a href=http://tutuorial.math.lamar.edu/ class=external-link target=_blank rel=noopener>http://tutuorial.math.lamar.edu/</a></p><h2 id=vector-spaces>vector spaces
<a class=heading-link href=#vector-spaces><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=linear-independence>linear independence
<a class=heading-link href=#linear-independence><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose \( S = {\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_n}} \) is a non-empty set of vectors and form the vector equation,
\[c_1\mathbf{v_1} +c_2\mathbf{v_2} + &mldr; +c_n\mathbf{v_n} =0\] This equation has at least one solution,namely, \(c_1 =0, c_2 =0,&mldr;, c_n =0\). This solution is called the <strong>trivial solution</strong>.</p><p>If the trivial solution is the only solution to this equation then the vectors in the set S are called <strong>linearly independent</strong> and the set is called a <strong>linearly independent set</strong>. If there is another solution then the vectors in the set S are called <strong>linearly dependent</strong> and the set is called a <strong>linearly dependent set</strong>.</p></li><li><p>A finite set of vectors that contains the zero vector will be linearly dependent.</p></li><li><p>Suppose that \( S = {\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_n}} \) is a set of vectors in \( \mathbb{R}^n\) . If k > n then the
set of vectors is linearly dependent.</p></li></ul><h3 id=basis>basis
<a class=heading-link href=#basis><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose \( S = {\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_n}} \) is a set of vectors from the vector space <em>V</em>. Then S is called a <strong>basis</strong> (plural is <strong>bases</strong>) for <em>V</em> if both of the following conditions hold.</p><p>(a) \(span(S)=V\), i.e. <em>S</em> spans the vector space <em>V</em>.</p><p>(b) <em>S</em> is a linearly independent set of vectors.</p></li><li><p>Suppose that the set \( S = {\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_n}} \) is a basis for the vector space <em>V</em> then
every vector <strong>u</strong> from <em>V</em> can be expressed as a linear combination of the vectors from <em>S</em> in exactly one way.</p></li></ul><h3 id=dimension>dimension
<a class=heading-link href=#dimension><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose that <strong>V</strong> is a non-zero vector space and that <strong>S</strong> is a set of vectors from <strong>V</strong> that for a basis for <strong>V</strong>. If <strong>S</strong> contains a finite number of vectors, say \( S = {\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_n}} \),
then we call <em>V</em> a <strong>finite dimensional</strong> vector space and we say that the <strong>dimension</strong> of <em>V</em>, denoted by dim (<em>V</em> ) , is n (i.e. the number of basis elements in <em>S</em>. If <em>V</em> is not a finite
dimensional vector space (so S does not have a finite number of vectors) then we call it an <strong>infinite dimensional</strong> vector space.</p><p>By definition the dimension of the zero vector space (i.e. the vector space consisting solely of the zero vector) is zero.</p></li><li><p>Suppose that <em>V</em> is a finite dimensional vector space with dim (<em>V</em> ) = <em>n</em> and that
<em>S</em> is any finite set of vectors from <em>V</em>.</p><p>(a) If <em>S</em> spans <em>V</em> but is not a basis for <em>V</em> then it can be reduced to a basis for <em>V</em> by
removing certain vectors from <em>S</em>.</p><p>(b) If <em>S</em> is linearly independent but is not a basis <em>V</em> then it can be enlarged to a
basis for <em>V</em> by adding in certain vectors from <em>V</em>.</p></li></ul><h3 id=change-of-basis>change of basis
<a class=heading-link href=#change-of-basis><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose that \( S = {\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_n}} \) is a basis for a vector space <em>V</em> and that <strong>u</strong> is any vector from <em>V</em>. Since <strong>u</strong> is a vector in <em>V</em> it can be expressed as a linear combination of
the vectors from <em>S</em> as follows,
\[ \mathbf{u} = c_1\mathbf{v_1} +c_2\mathbf{v_2} + &mldr; +c_n\mathbf{v_n} \]
The scalars \( c_1,c_2,&mldr;,c_n \) are called the <strong>coordinates</strong> of u relative to the basis <em>S</em>. The <strong>coordinate vectors of u relative to</strong> <em><strong>S</strong></em> is denoted by \( (u)_S \) and defined to be the following vector in \( \mathbb{R}^n \), \[ (\mathbf{u})_S =(c_1,c_2,&mldr;,c_n) \]</p></li><li><p>Suppose that <em>V</em> is a n-dimensional vector space and further suppose that \( B ={\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_n}} \) and \( C ={\mathbf{w_1},\mathbf{w_2},&mldr;,\mathbf{w_n}} \) are two bases for <em>V</em>. The <strong>transition matrix</strong> from <em>C</em> to <em>B</em> is defined to be,</p></li></ul><p>\[ P = [[\mathbf{w_1}_B]|[\mathbf{w_2}_B]|&mldr;|[\mathbf{w_n}_B]] \]</p><p>where the \(i^th\) column of <em>P</em> is the coordinate matrix of \(\mathbf{w_i}\) relative to <em>B</em>.</p><p>The coordinate matrix of a vector <strong>u</strong> in <em>V</em>, relative to <em>B</em>, is then related to the coordinate matrix of <strong>u</strong> relative to <em>C</em> by the following equation.
\[ {[\mathbf{u}]}_B =P{[\mathbf{u}]}_C \]</p><ul><li><p>Suppose that <em>V</em> is a finite dimensional vector space and that <em>P</em> is the transition matrix from <em>C</em> to <em>B</em> then,</p><p>(a) <em>P</em> is invertible and,</p><p>(b) \(P^{−1}\) is the transition matrix from <em>B</em> to <em>C</em>.</p></li></ul><h3 id=fundamental-subspaces>fundamental subspaces
<a class=heading-link href=#fundamental-subspaces><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>The <strong>row vectors</strong> (we called them row matrices at the time) are the vectors in \(\mathbb{R}^m\) formed out of the rows of <em>A</em>. The <strong>column vectors</strong> (again we called them column matrices at the time) are the vectors in \(\mathbb{R}^n\) that are formed out of the columns of <em>A</em>.</p></li><li><p>Suppose that <em>A</em> is an <em>n × m</em> matrix.</p><p>(a) The subspace of \(\mathbb{R}^m\) that is spanned by the row vectors of <em>A</em> is called the <strong>row space</strong> of <em>A</em>.</p><p>(b) The subspace of \(\mathbb{R}^n\) that is spanned by the column vectors of <em>A</em> is called the <strong>column space</strong> of <em>A</em>.</p></li><li><p>The dimension of the null space of <em>A</em> is called the <strong>nullity</strong> of <em>A</em> and is denoted by nullity(<em>A</em>).</p></li><li><p>Suppose that <em>A</em> is a matrix and <em>U</em> is a matrix in row-echelon form that has been obtained by performing row operations on <em>A</em>. Then the row space of <em>A</em> and the row space of <em>U</em> are the same space.</p></li><li><p>Suppose that <em>A</em> and <em>B</em> are two row equivalent matrices (so we got from one to the other by row operations) then a set of column vectors from <em>A</em> will be a basis for the column space of <em>A</em> if and only if the corresponding columns from <em>B</em> will form a basis for the column space of <em>B</em>.</p></li></ul><h3 id=rank>rank
<a class=heading-link href=#rank><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose that <em>A</em> is a matrix then the row space of <em>A</em> and the column space of <em>A</em> will have the same dimension. We call this common dimension the <strong>rank</strong> of A and denote it by rank(<em>A</em>).</p></li><li><p>Let A be an <em>n × n</em> matrix. The following statements are equivalent.</p><p>(a) <em>A</em> is invertible.</p><p>(b) The only solution to the system \(A\mathbf{x} = 0 \) is the trivial solution.</p><p>(c) <em>A</em> is row equivalent to \(I_n\) .</p><p>(d) <em>A</em> is expressible as a product of elementary matrices.</p><p>(e) \(A\mathbf{x} = \mathbf{b} \) has exactly one solution for every <em>n ×1</em> matrix <strong>b</strong>.</p><p>(f) \(A\mathbf{x} = \mathbf{b} \) is consistent for every <em>n ×1</em> matrix <strong>b</strong>.</p><p>(g) \( det(A) \neq 0\)</p><p>(h) The null space of A is {0}, i.e. just the zero vector.</p><p>(i) nullity(<em>A</em>)=0.</p><p>(j) rank(<em>A</em>)=<em>n</em>.</p><p>(k) The columns vectors of <em>A</em> form a basis for \(\mathbb{R}^n\).</p><p>(l) The row vectors of <em>A</em> form a basis for \(\mathbb{R}^n\).</p></li></ul><h3 id=inner-product>inner product
<a class=heading-link href=#inner-product><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose <strong>u</strong>, <strong>v</strong>, and <strong>w</strong> are all vectors in a vector space <em>V</em> and <em>c</em> is any scalar. An <strong>inner product</strong> on the vector space <em>V</em> is a function that associates with each pair of
vectors in <em>V</em>, say <strong>u</strong> and <strong>v</strong>, a real number \( &lt;\mathbf{u,v}> \)that satisfies the following denoted by axioms.</p><p>(a) \( &lt;\mathbf{u,v}> = &lt;\mathbf{v,u}> \)</p><p>(b) \( &lt;\mathbf{u+v,w}> = &lt;\mathbf{u,w}> + &lt;\mathbf{v,w}> \)</p><p>(c) \( &lt; {c}\mathbf{u,v}> = {c}&lt;\mathbf{u,v}> \)</p><p>(d) \( &lt;\mathbf{u,u}> \geq 0 \) and \( &lt;\mathbf{u,u}> =0 \) if and only if <strong>u</strong>=0</p><p>A vector space along with an inner product is called an <strong>inner product space</strong>.</p></li><li><p>Suppose that <strong>u</strong> and <strong>v</strong> are two vectors in an inner product space. They are said to be <strong>orthogonal</strong> if \( &lt;\mathbf{u,v}> =0 \).</p></li></ul><h3 id=orthogonal-complements>orthogonal complements
<a class=heading-link href=#orthogonal-complements><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose that <em>W</em> is a subspace of an inner product space <em>V</em>. We say that a vector <strong>u</strong> from <em>V</em> is <strong>orthogonal to W</strong> if it is orthogonal to every vector in <em>W</em>. The set of all vectors that are orthogonal to <em>W</em> is called the <strong>orthogonal complement of</strong> <em><strong>W</strong></em> and is denoted by \( W^{\bot} \).</p><p>We say that <em>W</em> and \( W^{\bot} \) are <strong>orthogonal complements</strong>.</p></li><li><p>Suppose <em>W</em> is a subspace of an inner product space <em>V</em>. Then,</p><p>(a) \( W^{\bot} \) is a subspace of <em>V</em>.</p><p>(b) Only the zero vector, <strong>0</strong>, is common to both <em>W</em> and \( W^{\bot} \) .</p><p>(c) \( {(W^{\bot})}^{\bot} = W \) . Or in other words, the orthogonal complement of \( W^{\bot} \) is W.</p></li></ul><h3 id=orthogonal-basis>orthogonal basis
<a class=heading-link href=#orthogonal-basis><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><p>Suppose that <em>S</em> is a set of vectors in an inner product space.</p><p>(a) If each pair of distinct vectors from <em>S</em> is orthogonal then we call <em>S</em> an <strong>orthogonal set</strong>.</p><p>(b) If <em>S</em> is an orthogonal set and each of the vectors in <em>S</em> also has a norm of 1 then we call <em>S</em> an <strong>orthonormal set</strong>.</p></li><li><p>Suppose that \( S ={ \mathbf{v_1}, \mathbf{v_2},&mldr; ,\mathbf{v_n}} \) is an orthogonal basis for an inner product space and that <strong>u</strong> is any vector from the inner product space then,</p></li></ul><p>\[ \mathbf{u} = { &lt; \mathbf{u,v_1} > \over {\parallel \mathbf{v_1} \parallel}^2 } \mathbf{v_1} + { &lt; \mathbf{u,v_2} > \over {\parallel \mathbf{v_2} \parallel}^2 } \mathbf{v_2} + &mldr; + { &lt; \mathbf{u,v_n} > \over {\parallel \mathbf{v_n} \parallel}^2 } \mathbf{v_n}\]</p><p>If in addition <em>S</em> is in fact an orthonormal basis then,</p><p>\[ \mathbf{u} = &lt;\mathbf{u,v_1}>\mathbf{v_1} + &lt;\mathbf{u,v_2}>\mathbf{v_2} + &mldr; + &lt;\mathbf{u,v_n}>\mathbf{v_n} \]</p><h3 id=gram-schmidt-porcess>Gram-Schmidt Porcess
<a class=heading-link href=#gram-schmidt-porcess><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Suppose that <em>V</em> is a finite dimensional inner product space and that \( {\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_n}} \) is a basis for <em>V</em> then an orthogonal basis for <em>V</em>, \( {\mathbf{u_1},\mathbf{u_2},&mldr;,\mathbf{u_n}} \), can be found using the following process.</p><p>\[ \mathbf{u_1} = \mathbf{v_1} \]</p><p>\[ \mathbf{u_2} = \mathbf{v_2} - {&lt;\mathbf{v_2, u_1}> \over {\parallel \mathbf{u_1} \parallel}^2}\mathbf{u_1} \]</p><p>\[ \mathbf{u_3} = \mathbf{v_3} - {&lt;\mathbf{v_3, u_1}> \over {\parallel \mathbf{u_1} \parallel}^2}\mathbf{u_1} - {&lt;\mathbf{v_3, u_2}> \over {\parallel \mathbf{u_2} \parallel}^2}\mathbf{u_2} \]</p><p>\[ \vdots \vdots \]</p><p>\[ \mathbf{u_n} = \mathbf{v_n} - {&lt;\mathbf{v_n, u_1}> \over {\parallel \mathbf{u_1} \parallel}^2}\mathbf{u_1} - {&lt;\mathbf{v_n, u_2}> \over {\parallel \mathbf{u_2} \parallel}^2}\mathbf{u_2} - {&lt;\mathbf{v_n, u_3}> \over {\parallel \mathbf{u_3} \parallel}^2}\mathbf{u_3} - &mldr; \]</p><p>To convert the basis to an orthonormal basis simply divide all the new basis vectors by their norm. Also, due to the construction process we have</p><p>\[ span(\mathbf{u_1},\mathbf{u_2},&mldr;,\mathbf{u_k}) = span(\mathbf{v_1},\mathbf{v_2},&mldr;,\mathbf{v_k}) \] for \[k =1,2,&mldr;,n \]</p><p>and \(u_k \) will be orthogonal to</p><p>\( span( v1, &mldr;,v_{k−1}) \) for \( k=2,3,&mldr;n. \)</p></div><footer></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
1990 -
2025
130l
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>